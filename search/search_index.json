{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"HPC Cost Simulator (HCS) HPC Cost Simulator (HCS) enables customers to simulate their High Performance Computing (HPC) costs on AWS. To simulate your future costs, HCS analyzes past job records (scheduler accounting database) and performs hour-by-hour cost analysis. Supported schedulers: IBM LSF SchedMD Slurm Altair Accelerator The output allows customers to: Estimate their Amazon Elastic Compute Cloud (Amazon EC2) costs hour-by-hour Choose the optimal consumption model to minimize costs, including Savings Plans , Reserved Instances and Spot Instances Get statistics about their workloads, locating specific job configurations that are waiting in queue longer, reducing engineering productivity. Data privacy HCS avoids the need to share scheduler logs with AWS. Scheduler logs include business sensitive data (tape out dates, project names) and PII (user names). Instead, customers run HCS on premises, creating an anonymized output that may be shared with your AWS account team to get additional cost optimization guidance. The output file (located under the output/ directory if not otherwise specified) is a simple Excel (.xslx), allowing you to audit its contents before sharing it. How data is analyzed The analysis is a 4-step process: # Step Description 1 Data Collection Collect accounting records from the database. 2 Formatting Parsing the scheduler accounting database - this takes the scheduler specific format, and converts the job records to a uniform format (not scheduler specific). 3 Cost Simulation Performing cost simulation using the output from step 2. 4 Analysis Analyzing the output, building a complete cost model. This can be done using your AWS account team. Prerequisites Python 3.6 or newer sudo access to install any required tools Recommended: CentOS 7 or higher A bash setup script is provided to install the required packages and create a Python virtual environment. The pip packages and versions are listed in requirements.txt . The setup script must be sourced, not executed, to set up the environment to run the tool. source setup.sh Configuration HCS comes pre-configured with a few best practices for analyzing an Electronics Design Automation (EDA) workload. You can edit the configuration by editing config.yml before running HCS. We encourage you to read the file before running to tool, to know what parameters are used to simulate the workload. A partial list of these configuration parameters includes: AWS region Instance selection guidelines Savings Plans selection guidelines Spot selection guidelines Instance Type Information To perform cost simulation, HCS requires a list of available instance types in the region and their prices that is stored in instance_type_info.json . HCS comes pre-populated with the database, and we periodically update it to reflect new instance types, prices etc. However, to update it yourself , see Updating Instance Type Information Running the Tool Steps 1 & 2 These steps are scheduler-specific and are explained in each scheduler's documentation page: IBM LSF SchedMD Slurm Altair Engineering Accelerator If you want to parse data from an unsupported scheduler, you can export the accounting data from that scheduler generating your own CSV, as long as you CSV structure defined in Understanding the CSV format Step 3 Once completed, you can run step #3 (cost simulation) by following the instructions in for the JobAnalyzer Notes for step #3 The job analyzer can call the parser (step 2), however, parsing is the longest step so it is recommended that you run it separately. The job analyzer can be run with different configuration file ( config.yml ) on the same job data. For example: Simulating your workload costs with and without Spot Instances Simulating your costs with different instance families Step 4 For analysis, you can either share the Excel file from step #3 with your AWS account team, or read the Output page for more details.","title":"HPC Cost Simulator (HCS)"},{"location":"#hpc-cost-simulator-hcs","text":"HPC Cost Simulator (HCS) enables customers to simulate their High Performance Computing (HPC) costs on AWS. To simulate your future costs, HCS analyzes past job records (scheduler accounting database) and performs hour-by-hour cost analysis. Supported schedulers: IBM LSF SchedMD Slurm Altair Accelerator The output allows customers to: Estimate their Amazon Elastic Compute Cloud (Amazon EC2) costs hour-by-hour Choose the optimal consumption model to minimize costs, including Savings Plans , Reserved Instances and Spot Instances Get statistics about their workloads, locating specific job configurations that are waiting in queue longer, reducing engineering productivity.","title":"HPC Cost Simulator (HCS)"},{"location":"#data-privacy","text":"HCS avoids the need to share scheduler logs with AWS. Scheduler logs include business sensitive data (tape out dates, project names) and PII (user names). Instead, customers run HCS on premises, creating an anonymized output that may be shared with your AWS account team to get additional cost optimization guidance. The output file (located under the output/ directory if not otherwise specified) is a simple Excel (.xslx), allowing you to audit its contents before sharing it.","title":"Data privacy"},{"location":"#how-data-is-analyzed","text":"The analysis is a 4-step process: # Step Description 1 Data Collection Collect accounting records from the database. 2 Formatting Parsing the scheduler accounting database - this takes the scheduler specific format, and converts the job records to a uniform format (not scheduler specific). 3 Cost Simulation Performing cost simulation using the output from step 2. 4 Analysis Analyzing the output, building a complete cost model. This can be done using your AWS account team.","title":"How data is analyzed"},{"location":"#prerequisites","text":"Python 3.6 or newer sudo access to install any required tools Recommended: CentOS 7 or higher A bash setup script is provided to install the required packages and create a Python virtual environment. The pip packages and versions are listed in requirements.txt . The setup script must be sourced, not executed, to set up the environment to run the tool. source setup.sh","title":"Prerequisites"},{"location":"#configuration","text":"HCS comes pre-configured with a few best practices for analyzing an Electronics Design Automation (EDA) workload. You can edit the configuration by editing config.yml before running HCS. We encourage you to read the file before running to tool, to know what parameters are used to simulate the workload. A partial list of these configuration parameters includes: AWS region Instance selection guidelines Savings Plans selection guidelines Spot selection guidelines","title":"Configuration"},{"location":"#instance-type-information","text":"To perform cost simulation, HCS requires a list of available instance types in the region and their prices that is stored in instance_type_info.json . HCS comes pre-populated with the database, and we periodically update it to reflect new instance types, prices etc. However, to update it yourself , see Updating Instance Type Information","title":"Instance Type Information"},{"location":"#running-the-tool","text":"","title":"Running the Tool"},{"location":"#steps-1-2","text":"These steps are scheduler-specific and are explained in each scheduler's documentation page: IBM LSF SchedMD Slurm Altair Engineering Accelerator If you want to parse data from an unsupported scheduler, you can export the accounting data from that scheduler generating your own CSV, as long as you CSV structure defined in Understanding the CSV format","title":"Steps 1 &amp; 2"},{"location":"#step-3","text":"Once completed, you can run step #3 (cost simulation) by following the instructions in for the JobAnalyzer Notes for step #3 The job analyzer can call the parser (step 2), however, parsing is the longest step so it is recommended that you run it separately. The job analyzer can be run with different configuration file ( config.yml ) on the same job data. For example: Simulating your workload costs with and without Spot Instances Simulating your costs with different instance families","title":"Step 3"},{"location":"#step-4","text":"For analysis, you can either share the Excel file from step #3 with your AWS account team, or read the Output page for more details.","title":"Step 4"},{"location":"AcceleratorLogParser/","text":"Altair Accelerator The AcceleratorLogParser.py script queries the Accelerator results database, parses the job completion data, and saves it into a CSV file. Collecting the data To see data collection command syntax, run: source setup.sh ./AcceleratorLogParser.py --show-data-collection-cmd For example, as of this writing, the command is: nc cmd vovsql_query -e \"select jobs.id, jobs.submittime, jobs.starttime, jobs.endtime, resources.name, jobs.exitstatus, jobs.maxram, jobs.maxvm, jobs.cputime, jobs.susptime from jobs inner join resources on jobs.resourcesid=resources.id\" > sql-output.txt This command should run on a node connected to the Altair Accelerator head node, and with a user with permissions to see the jobs from all users. Note: To be able to run the command, you first need to source the vovrc.sh script located in your /common/etc subfolder of your Altair install folder. Parsing the Job Completion Data The parsed data will be written in the output directory which will be created if it does not exist. ./AcceleratorLogParser.py \\ --sql-output-file sql-output.txt \\ --output-csv output/jobs.csv Full Syntax usage: AcceleratorLogParser.py [-h] (--show-data-collection-cmd | --sql-output-file SQL_OUTPUT_FILE | --sql-input-file SQL_INPUT_FILE) [--output-csv OUTPUT_CSV] [--default-mem-gb DEFAULT_MEM_GB] [--starttime STARTTIME] [--endtime ENDTIME] [--debug] Parse Altair Accelerator logs. optional arguments: -h, --help show this help message and exit --show-data-collection-cmd Show the command to create the SQL_OUTPUT_FILE. (default: False) --sql-output-file SQL_OUTPUT_FILE File where the output of sql query will be written. Cannot be used with --sql-input-file. Required if --sql-input-file not set. (default: None) --sql-input-file SQL_INPUT_FILE File with the output of sql query so can process it offline. Cannot be used with --sql-output-file. Required if --sql-output-file not set. (default: None) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --default-mem-gb DEFAULT_MEM_GB Default amount of memory (in GB) requested for jobs that do not have a memory request. (default: 0.0) --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --debug, -d Enable debug mode (default: False) What's next? Once completed, you can run step #3 (cost simulation) by following the instructions in for the JobAnalyzer Since you already generated a CSV file, you will use JobAnalyzer with the csv option.","title":"Altair Accelerator"},{"location":"AcceleratorLogParser/#altair-accelerator","text":"The AcceleratorLogParser.py script queries the Accelerator results database, parses the job completion data, and saves it into a CSV file.","title":"Altair Accelerator"},{"location":"AcceleratorLogParser/#collecting-the-data","text":"To see data collection command syntax, run: source setup.sh ./AcceleratorLogParser.py --show-data-collection-cmd For example, as of this writing, the command is: nc cmd vovsql_query -e \"select jobs.id, jobs.submittime, jobs.starttime, jobs.endtime, resources.name, jobs.exitstatus, jobs.maxram, jobs.maxvm, jobs.cputime, jobs.susptime from jobs inner join resources on jobs.resourcesid=resources.id\" > sql-output.txt This command should run on a node connected to the Altair Accelerator head node, and with a user with permissions to see the jobs from all users. Note: To be able to run the command, you first need to source the vovrc.sh script located in your /common/etc subfolder of your Altair install folder.","title":"Collecting the data"},{"location":"AcceleratorLogParser/#parsing-the-job-completion-data","text":"The parsed data will be written in the output directory which will be created if it does not exist. ./AcceleratorLogParser.py \\ --sql-output-file sql-output.txt \\ --output-csv output/jobs.csv","title":"Parsing the Job Completion Data"},{"location":"AcceleratorLogParser/#full-syntax","text":"usage: AcceleratorLogParser.py [-h] (--show-data-collection-cmd | --sql-output-file SQL_OUTPUT_FILE | --sql-input-file SQL_INPUT_FILE) [--output-csv OUTPUT_CSV] [--default-mem-gb DEFAULT_MEM_GB] [--starttime STARTTIME] [--endtime ENDTIME] [--debug] Parse Altair Accelerator logs. optional arguments: -h, --help show this help message and exit --show-data-collection-cmd Show the command to create the SQL_OUTPUT_FILE. (default: False) --sql-output-file SQL_OUTPUT_FILE File where the output of sql query will be written. Cannot be used with --sql-input-file. Required if --sql-input-file not set. (default: None) --sql-input-file SQL_INPUT_FILE File with the output of sql query so can process it offline. Cannot be used with --sql-output-file. Required if --sql-output-file not set. (default: None) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --default-mem-gb DEFAULT_MEM_GB Default amount of memory (in GB) requested for jobs that do not have a memory request. (default: 0.0) --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --debug, -d Enable debug mode (default: False)","title":"Full Syntax"},{"location":"AcceleratorLogParser/#whats-next","text":"Once completed, you can run step #3 (cost simulation) by following the instructions in for the JobAnalyzer Since you already generated a CSV file, you will use JobAnalyzer with the csv option.","title":"What's next?"},{"location":"JobAnalyzer/","text":"Job Analyzer The JobAnalyzer.py performs the cost simulation using the outputs from AcceleratorLogParser.py , LSFLogParser.py or SlurmLogParser.py It produces an hour-by-hour cost simulation, placing the output in the output/ subfolder (by default). For convenience, the analyzer can call the parser and analyze the output in 1 step, however we recommend performing the analysis in separate stages (see \"How data is analyzed\") in index.md . Prerequisites JobAnalyzer.py relies on: the virtual environment created by the setup script. run source setup.sh to setup the virtual environment. config.yml which defines the configuration of the analysis. For more details on the configuration file, see the configuration documentation instance_type_info.json which contains instance type details and pricing. The file is part of the repository, but if you want to download an update list of instances and their prices, please see Updating the Instance Type Information Performing the cost simulation Once you generated a CSV file from your Scheduler (See instructions for IBM LSF , SchedMD Slurm abd Altair Engineering Accelerator ) you can perform the cost simulation (step 3) using JobAnalyzer.py To parse the CSV file into the final anonymized Excel report, run: source setup.sh ./JobAnalyzer.py csv --input-csv INPUT_CSV_FILE_NAME Outputs By default, HCS places all output files in the output/ folder (this can be changed using the --output-dir parameter to JobAnalyzer.py ). Note: The output folder will get overwritten without prompting you for approval. Full Syntax Arguments provided to JobAnalyzer are required in a specific order: ./JobAnalyzer.py <Arguments that apply to all schedulers> <Parser type> <Parser-specific Arguments> These are the common arguments. usage: JobAnalyzer.py [-h] [--starttime STARTTIME] [--endtime ENDTIME] [--config CONFIG] [--acknowledge-config] [--output-dir OUTPUT_DIR] [--output-csv OUTPUT_CSV] [--debug] parser ... Analyze jobs positional arguments: parser Choose the kind of information to parse. ./JobAnalyzer.py <parser> -h for parser specific arguments. accelerator Parse Accelerator (nc) job information csv Parse CSV from already parsed job information. lsf Parse LSF logfiles slurm Parse Slurm job information hourly_stats Parse hourly_stats file so can create Excel workbook (xlsx). optional arguments: -h, --help show this help message and exit --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --config CONFIG Configuration file. (default: ./config.yml) --acknowledge-config Acknowledge configuration file contents so don't get prompt. (default: False) --output-dir OUTPUT_DIR Directory where output will be written (default: output) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --debug, -d Enable debug mode (default: False) Arguments that apply to all schedulers When used, these parameters must precede the parser type: --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM-DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM-DDTHH:MM:SS (default: None) --config CONFIG Configuration file. (default: ./config.yml) --output-dir OUTPUT_DIR Directory where output will be written (default: output) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --debug, -d Enable debug mode (default: False) --help, -h Show help message Parser Type The tool supports 5 parser types: accelerator Parse Accelerator (nc) job information lsf Parse LSF accounting records (lsb.acct fields) slurm Parse Slurm job information csv Parse CSV from a previously parsed job information. hourly_stats Parse the hourly output files from a previous run Parser-Specific Arguments - Accelerator usage: JobAnalyzer.py accelerator [-h] [--default-mem-gb DEFAULT_MEM_GB] (--sql-output-file SQL_OUTPUT_FILE | --sql-input-file SQL_INPUT_FILE) optional arguments: -h, --help show this help message and exit --default-mem-gb DEFAULT_MEM_GB Default amount of memory (in GB) requested for jobs. (default: 0.0) --sql-output-file SQL_OUTPUT_FILE File where the output of sql query will be written. Cannot be used with --sql-input-file. Required if --sql-input-file not set. Command to create file: nc cmd vovsql_query -e \"select jobs.id, jobs.submittime, jobs.starttime, jobs.endtime, resources.name, jobs.exitstatus, jobs.maxram, jobs.maxvm, jobs.cputime, jobs.susptime from jobs inner join resources on jobs.resourcesid=resources.id\" > SQL_OUTPUT_FILE (default: None) --sql-input-file SQL_INPUT_FILE File with the output of sql query so can process it offline. Cannot be used with --sql-output-file. Required if --sql-output-file not set. (default: None) Parser-Specific Arguments - LSF usage: JobAnalyzer.py lsf [-h] [--logfile-dir LOGFILE_DIR] --default-max-mem-gb DEFAULT_MAX_MEM_GB optional arguments: -h, --help show this help message and exit --logfile-dir LOGFILE_DIR LSF logfile directory (default: None) --default-max-mem-gb DEFAULT_MAX_MEM_GB Default maximum memory for a job in GB. (default: None) Parser-Specific Arguments - Slurm usage: JobAnalyzer.py slurm [-h] [--slurm-root SLURM_ROOT] [--sacct-output-file SACCT_OUTPUT_FILE | --sacct-input-file SACCT_INPUT_FILE] optional arguments: -h, --help show this help message and exit --slurm-root SLURM_ROOT Directory that contains the Slurm bin directory. (default: None) --sacct-output-file SACCT_OUTPUT_FILE File where the output of sacct will be written. Cannot be used with --sacct-input-file. Required if --sacct- input-file not set. (default: None) --sacct-input-file SACCT_INPUT_FILE File with the output of sacct so can process sacct output offline. Cannot be used with --sacct-output- file. Required if --sacct-output-file not set. (default: None) Note: The tool will call sacct to get accounting logs, if you don't have it installed on the machine, please see the SlurmLogParser.py documentation for details on how to save them to a CSV file. What's Next? Once completed, you can find your Excel report under output/hourly_stats.xlsx . You can share it with your AWS account team (Recommended) for further cost optimization guidance, or you can learn more about the data in the Excel in the Output documentation page.","title":"Job Analyzer"},{"location":"JobAnalyzer/#job-analyzer","text":"The JobAnalyzer.py performs the cost simulation using the outputs from AcceleratorLogParser.py , LSFLogParser.py or SlurmLogParser.py It produces an hour-by-hour cost simulation, placing the output in the output/ subfolder (by default). For convenience, the analyzer can call the parser and analyze the output in 1 step, however we recommend performing the analysis in separate stages (see \"How data is analyzed\") in index.md .","title":"Job Analyzer"},{"location":"JobAnalyzer/#prerequisites","text":"JobAnalyzer.py relies on: the virtual environment created by the setup script. run source setup.sh to setup the virtual environment. config.yml which defines the configuration of the analysis. For more details on the configuration file, see the configuration documentation instance_type_info.json which contains instance type details and pricing. The file is part of the repository, but if you want to download an update list of instances and their prices, please see Updating the Instance Type Information","title":"Prerequisites"},{"location":"JobAnalyzer/#performing-the-cost-simulation","text":"Once you generated a CSV file from your Scheduler (See instructions for IBM LSF , SchedMD Slurm abd Altair Engineering Accelerator ) you can perform the cost simulation (step 3) using JobAnalyzer.py To parse the CSV file into the final anonymized Excel report, run: source setup.sh ./JobAnalyzer.py csv --input-csv INPUT_CSV_FILE_NAME","title":"Performing the cost simulation"},{"location":"JobAnalyzer/#outputs","text":"By default, HCS places all output files in the output/ folder (this can be changed using the --output-dir parameter to JobAnalyzer.py ). Note: The output folder will get overwritten without prompting you for approval.","title":"Outputs"},{"location":"JobAnalyzer/#full-syntax","text":"Arguments provided to JobAnalyzer are required in a specific order: ./JobAnalyzer.py <Arguments that apply to all schedulers> <Parser type> <Parser-specific Arguments> These are the common arguments. usage: JobAnalyzer.py [-h] [--starttime STARTTIME] [--endtime ENDTIME] [--config CONFIG] [--acknowledge-config] [--output-dir OUTPUT_DIR] [--output-csv OUTPUT_CSV] [--debug] parser ... Analyze jobs positional arguments: parser Choose the kind of information to parse. ./JobAnalyzer.py <parser> -h for parser specific arguments. accelerator Parse Accelerator (nc) job information csv Parse CSV from already parsed job information. lsf Parse LSF logfiles slurm Parse Slurm job information hourly_stats Parse hourly_stats file so can create Excel workbook (xlsx). optional arguments: -h, --help show this help message and exit --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --config CONFIG Configuration file. (default: ./config.yml) --acknowledge-config Acknowledge configuration file contents so don't get prompt. (default: False) --output-dir OUTPUT_DIR Directory where output will be written (default: output) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --debug, -d Enable debug mode (default: False)","title":"Full Syntax"},{"location":"JobAnalyzer/#arguments-that-apply-to-all-schedulers","text":"When used, these parameters must precede the parser type: --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM-DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM-DDTHH:MM:SS (default: None) --config CONFIG Configuration file. (default: ./config.yml) --output-dir OUTPUT_DIR Directory where output will be written (default: output) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --debug, -d Enable debug mode (default: False) --help, -h Show help message","title":"Arguments that apply to all schedulers"},{"location":"JobAnalyzer/#parser-type","text":"The tool supports 5 parser types: accelerator Parse Accelerator (nc) job information lsf Parse LSF accounting records (lsb.acct fields) slurm Parse Slurm job information csv Parse CSV from a previously parsed job information. hourly_stats Parse the hourly output files from a previous run","title":"Parser Type"},{"location":"JobAnalyzer/#parser-specific-arguments-accelerator","text":"usage: JobAnalyzer.py accelerator [-h] [--default-mem-gb DEFAULT_MEM_GB] (--sql-output-file SQL_OUTPUT_FILE | --sql-input-file SQL_INPUT_FILE) optional arguments: -h, --help show this help message and exit --default-mem-gb DEFAULT_MEM_GB Default amount of memory (in GB) requested for jobs. (default: 0.0) --sql-output-file SQL_OUTPUT_FILE File where the output of sql query will be written. Cannot be used with --sql-input-file. Required if --sql-input-file not set. Command to create file: nc cmd vovsql_query -e \"select jobs.id, jobs.submittime, jobs.starttime, jobs.endtime, resources.name, jobs.exitstatus, jobs.maxram, jobs.maxvm, jobs.cputime, jobs.susptime from jobs inner join resources on jobs.resourcesid=resources.id\" > SQL_OUTPUT_FILE (default: None) --sql-input-file SQL_INPUT_FILE File with the output of sql query so can process it offline. Cannot be used with --sql-output-file. Required if --sql-output-file not set. (default: None)","title":"Parser-Specific Arguments - Accelerator"},{"location":"JobAnalyzer/#parser-specific-arguments-lsf","text":"usage: JobAnalyzer.py lsf [-h] [--logfile-dir LOGFILE_DIR] --default-max-mem-gb DEFAULT_MAX_MEM_GB optional arguments: -h, --help show this help message and exit --logfile-dir LOGFILE_DIR LSF logfile directory (default: None) --default-max-mem-gb DEFAULT_MAX_MEM_GB Default maximum memory for a job in GB. (default: None)","title":"Parser-Specific Arguments - LSF"},{"location":"JobAnalyzer/#parser-specific-arguments-slurm","text":"usage: JobAnalyzer.py slurm [-h] [--slurm-root SLURM_ROOT] [--sacct-output-file SACCT_OUTPUT_FILE | --sacct-input-file SACCT_INPUT_FILE] optional arguments: -h, --help show this help message and exit --slurm-root SLURM_ROOT Directory that contains the Slurm bin directory. (default: None) --sacct-output-file SACCT_OUTPUT_FILE File where the output of sacct will be written. Cannot be used with --sacct-input-file. Required if --sacct- input-file not set. (default: None) --sacct-input-file SACCT_INPUT_FILE File with the output of sacct so can process sacct output offline. Cannot be used with --sacct-output- file. Required if --sacct-output-file not set. (default: None) Note: The tool will call sacct to get accounting logs, if you don't have it installed on the machine, please see the SlurmLogParser.py documentation for details on how to save them to a CSV file.","title":"Parser-Specific Arguments - Slurm"},{"location":"JobAnalyzer/#whats-next","text":"Once completed, you can find your Excel report under output/hourly_stats.xlsx . You can share it with your AWS account team (Recommended) for further cost optimization guidance, or you can learn more about the data in the Excel in the Output documentation page.","title":"What's Next?"},{"location":"LSFLogParser/","text":"IBM Spectrum LSF The LSFLogParser.py script parses the lsb.acct* files in the LSF logfile directory and writes the parsed data into a CSV file that can be read by JobAnalyzer.py . Parsing the accounting log files First you must source the setup script to make sure that all required packages are installed. source setup.sh This creates a python virtual environment and activates it. The script will parse all of the files that start with lsb.acct* in the logfile directory. The parsed data will be written in the output directory which will be created if it does not exist. The parsed job data will be written out in csv format. ./LSFLogParser.py \\ --logfile-dir <logfile-dir> \\ --output-csv jobs.csv Note: The parser expects the file names to start with lsb.acct , do not rename the files. Full Syntax usage: LSFLogParser.py [-h] --logfile-dir LOGFILE_DIR --output-csv OUTPUT_CSV [--default-max-mem-gb DEFAULT_MAX_MEM_GB] [--starttime STARTTIME] [--endtime ENDTIME] [--debug] Parse LSF logs. optional arguments: -h, --help show this help message and exit --logfile-dir LOGFILE_DIR LSF logfile directory (default: None) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --default-max-mem-gb DEFAULT_MAX_MEM_GB Default maximum memory for a job in GB. (default: 0.0) --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --debug, -d Enable debug mode (default: False) What's next? Once completed, you can run step #3 (cost simulation) by following the instructions in for the JobAnalyzer Since you already generated a CSV file, you will use JobAnalyzer with the csv option.","title":"IBM Spectrum LSF"},{"location":"LSFLogParser/#ibm-spectrum-lsf","text":"The LSFLogParser.py script parses the lsb.acct* files in the LSF logfile directory and writes the parsed data into a CSV file that can be read by JobAnalyzer.py .","title":"IBM Spectrum LSF"},{"location":"LSFLogParser/#parsing-the-accounting-log-files","text":"First you must source the setup script to make sure that all required packages are installed. source setup.sh This creates a python virtual environment and activates it. The script will parse all of the files that start with lsb.acct* in the logfile directory. The parsed data will be written in the output directory which will be created if it does not exist. The parsed job data will be written out in csv format. ./LSFLogParser.py \\ --logfile-dir <logfile-dir> \\ --output-csv jobs.csv Note: The parser expects the file names to start with lsb.acct , do not rename the files.","title":"Parsing the accounting log files"},{"location":"LSFLogParser/#full-syntax","text":"usage: LSFLogParser.py [-h] --logfile-dir LOGFILE_DIR --output-csv OUTPUT_CSV [--default-max-mem-gb DEFAULT_MAX_MEM_GB] [--starttime STARTTIME] [--endtime ENDTIME] [--debug] Parse LSF logs. optional arguments: -h, --help show this help message and exit --logfile-dir LOGFILE_DIR LSF logfile directory (default: None) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --default-max-mem-gb DEFAULT_MAX_MEM_GB Default maximum memory for a job in GB. (default: 0.0) --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --debug, -d Enable debug mode (default: False)","title":"Full Syntax"},{"location":"LSFLogParser/#whats-next","text":"Once completed, you can run step #3 (cost simulation) by following the instructions in for the JobAnalyzer Since you already generated a CSV file, you will use JobAnalyzer with the csv option.","title":"What's next?"},{"location":"Output/","text":"Output HCS generates multiple output files: File Name Quantity Description hourly_stats.xlsx 1 The full anonymized output, the only file you'll share with your AWS account team (should you choose to do so hourly-######.csv 1 per hour Temporary files, includes all the jobs that started in a specific hour summary.csv 1 Aggregated summary of all your jobs (statistics). This data also appears in the Excel output file hourly_stats.csv 1 Host simulation output, includes. This data also appears in the Excel output file. JobAnalyzer-####-##-##_##-##-##.log 1 Simulation log file for trouble shooting hourly_stats.xlsx This is the main output file, and will be analyzed moving forward. All other files are only used for troubleshooting and or re-running a subset of the analysis. The Excel spreadsheet provides a convenient way to view the data and perform calculations on it. For example, in the following example the \" First hour to analyze \" was changed to only include the last 12 months worth of data. The InstanceFamilySummary worksheet can be used to get insights into the predicted usage of different instance families. In this example it shows that the c6i family is used the most with very little utilization of other instance families. The average hourly use can be used to help choose the upper constraint for the savings plans. Savings Plan Optimization hourly_stats.xlsx allows you to optimize your costs further by simulating the impact of Savings Plans (SP) on your overall costs, and finding the optimal SP cost for your workload. To do this, you will need to enable Excel's numerical solver add-in. To use the solver, you first need to enable it. Windows: Select File -> Options Select Add-ins on the lower left. If the Solver Add-in isn't active then select Manage: Excel Add-ins and click Go . Select the Solver Add-in and click OK . If the Solver Add-in is already active then it will show up like this. Mac OS: Select Tools -> Options If the Solver Add-in isn't active then select it and click OK . Optimize Savings Plans Using Excel Solver Now you are ready to run the solver. Copy the original values by selecting column B and pasting the values into column C (This will allow you to compare before vs. after cost optimization). Then select the Data menu select Solver on the ribbon. Configure the solver by selecting the total cost as the Objective . Then select the Savings Plan commit cells in By Changing Variable Cells . Do not select the total cell, only the $ value for specific instance family types: Optionally, you may add constraints for the Savings Plan commit cells. Windows: Mac OS Then click Solve and wait while the Solver calculates the savings plan commits that will minimize your overall costs. The solver may run a while, but when it finishes then save the results. Windows: Mac OS: The spreadsheet will then show the savings plan commits that minimize the total costs. In this case Savings Plans were able to reduce total costs by about 10%. Note that this is due to the very variable nature of the jobs. The more uniform usage you have, the more cost-effective Savings Plans will be. This graph shows the variability of the workload.","title":"Output"},{"location":"Output/#output","text":"HCS generates multiple output files: File Name Quantity Description hourly_stats.xlsx 1 The full anonymized output, the only file you'll share with your AWS account team (should you choose to do so hourly-######.csv 1 per hour Temporary files, includes all the jobs that started in a specific hour summary.csv 1 Aggregated summary of all your jobs (statistics). This data also appears in the Excel output file hourly_stats.csv 1 Host simulation output, includes. This data also appears in the Excel output file. JobAnalyzer-####-##-##_##-##-##.log 1 Simulation log file for trouble shooting","title":"Output"},{"location":"Output/#hourly_statsxlsx","text":"This is the main output file, and will be analyzed moving forward. All other files are only used for troubleshooting and or re-running a subset of the analysis. The Excel spreadsheet provides a convenient way to view the data and perform calculations on it. For example, in the following example the \" First hour to analyze \" was changed to only include the last 12 months worth of data. The InstanceFamilySummary worksheet can be used to get insights into the predicted usage of different instance families. In this example it shows that the c6i family is used the most with very little utilization of other instance families. The average hourly use can be used to help choose the upper constraint for the savings plans.","title":"hourly_stats.xlsx"},{"location":"Output/#savings-plan-optimization","text":"hourly_stats.xlsx allows you to optimize your costs further by simulating the impact of Savings Plans (SP) on your overall costs, and finding the optimal SP cost for your workload. To do this, you will need to enable Excel's numerical solver add-in. To use the solver, you first need to enable it.","title":"Savings Plan Optimization"},{"location":"Output/#windows","text":"Select File -> Options Select Add-ins on the lower left. If the Solver Add-in isn't active then select Manage: Excel Add-ins and click Go . Select the Solver Add-in and click OK . If the Solver Add-in is already active then it will show up like this.","title":"Windows:"},{"location":"Output/#mac-os","text":"Select Tools -> Options If the Solver Add-in isn't active then select it and click OK .","title":"Mac OS:"},{"location":"Output/#optimize-savings-plans-using-excel-solver","text":"Now you are ready to run the solver. Copy the original values by selecting column B and pasting the values into column C (This will allow you to compare before vs. after cost optimization). Then select the Data menu select Solver on the ribbon. Configure the solver by selecting the total cost as the Objective . Then select the Savings Plan commit cells in By Changing Variable Cells . Do not select the total cell, only the $ value for specific instance family types: Optionally, you may add constraints for the Savings Plan commit cells. Windows: Mac OS Then click Solve and wait while the Solver calculates the savings plan commits that will minimize your overall costs. The solver may run a while, but when it finishes then save the results. Windows: Mac OS: The spreadsheet will then show the savings plan commits that minimize the total costs. In this case Savings Plans were able to reduce total costs by about 10%. Note that this is due to the very variable nature of the jobs. The more uniform usage you have, the more cost-effective Savings Plans will be. This graph shows the variability of the workload.","title":"Optimize Savings Plans Using Excel Solver"},{"location":"SlurmLogParser/","text":"Slurm SlurmLogParser.py parses the Slurm Accounting Database records, and its output is then used by JobAnalyzer.py for cost simulation. Modes of running the tool SlurmLogParser.py has 2 modes of operation, online and offline. Offline - Recommended In offline mode, the job completion data is extracted using sacct ahead of processing (not as part of the script execution). This allows processing to happen on a Linux machine not configured as part of the Slurm cluster (non production machine). When running in this mode, provide the file path to HCS using the --sacct-input-file argument. Online Online means HCS will call the sacct command directly, which requires the Linux machine to have sacct installed and configured as well as the permissions to access accounting records from all users . Running in this mode will store the output of the sacct command in a CSV file to allow additional analysis to be done offline (minimizing calls the Slurm head node). You need to provide the name of this CSV file using the --sacct-output-file argument. If the Slurm bin folder is not in your path, you will also need to provide the --slurm-root argument, pointing to the Slurm bin folder. Note : You can't use both --sacct-input-file and --sacct-output-file together. Collecting the data To see data collection command syntax, run: source setup.sh ./SlurmLogParser.py --show-data-collection-cmd As of this writing, the command is: sacct --allusers --parsable2 --noheader --format State,JobID,ReqCPUS,ReqMem,ReqNodes,Constraints,Submit,Eligible,Start,Elapsed,Suspended,End,ExitCode,DerivedExitCode,AllocNodes,NCPUS,MaxDiskRead,MaxDiskWrite,MaxPages,MaxRSS,MaxVMSize,CPUTime,UserCPU,SystemCPU,TotalCPU,Partition --starttime 1970-01-01T0:00:00 > SlurmAccounting.csv Parsing the Job Completion Data First you must source the setup script to activate the virtual environment and install all the dependencies in it. source setup.sh ./SlurmLogParser.py \\ --sacct-input-file SlurmAccounting.csv \\ --output-csv jobs.csv Full Syntax usage: SlurmLogParser.py [-h] (--show-data-collection-cmd | --sacct-output-file SACCT_OUTPUT_FILE | --sacct-input-file SACCT_INPUT_FILE) [--output-csv OUTPUT_CSV] [--slurm-root SLURM_ROOT] [--starttime STARTTIME] [--endtime ENDTIME] [--debug] Parse Slurm logs. optional arguments: -h, --help show this help message and exit --show-data-collection-cmd Show the command to create SACCT_OUTPUT_FILE. (default: False) --sacct-output-file SACCT_OUTPUT_FILE File where the output of sacct will be written. Cannot be used with --sacct-file. Required if --sacct-input- file not set. (default: None) --sacct-input-file SACCT_INPUT_FILE File with the output of sacct so can process sacct output offline. Cannot be used with --sacct-output- file. Required if --sacct-output-file not set. (default: None) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --slurm-root SLURM_ROOT Directory that contains the Slurm bin directory. (default: None) --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --debug, -d Enable debug mode (default: False) What's next? Once completed, you can run step #3 (cost simulation) by following the instructions for the JobAnalyzer . Since you already generated a CSV file, you will use JobAnalyzer with the csv option.","title":"Slurm"},{"location":"SlurmLogParser/#slurm","text":"SlurmLogParser.py parses the Slurm Accounting Database records, and its output is then used by JobAnalyzer.py for cost simulation.","title":"Slurm"},{"location":"SlurmLogParser/#modes-of-running-the-tool","text":"SlurmLogParser.py has 2 modes of operation, online and offline.","title":"Modes of running the tool"},{"location":"SlurmLogParser/#offline-recommended","text":"In offline mode, the job completion data is extracted using sacct ahead of processing (not as part of the script execution). This allows processing to happen on a Linux machine not configured as part of the Slurm cluster (non production machine). When running in this mode, provide the file path to HCS using the --sacct-input-file argument.","title":"Offline - Recommended"},{"location":"SlurmLogParser/#online","text":"Online means HCS will call the sacct command directly, which requires the Linux machine to have sacct installed and configured as well as the permissions to access accounting records from all users . Running in this mode will store the output of the sacct command in a CSV file to allow additional analysis to be done offline (minimizing calls the Slurm head node). You need to provide the name of this CSV file using the --sacct-output-file argument. If the Slurm bin folder is not in your path, you will also need to provide the --slurm-root argument, pointing to the Slurm bin folder. Note : You can't use both --sacct-input-file and --sacct-output-file together.","title":"Online"},{"location":"SlurmLogParser/#collecting-the-data","text":"To see data collection command syntax, run: source setup.sh ./SlurmLogParser.py --show-data-collection-cmd As of this writing, the command is: sacct --allusers --parsable2 --noheader --format State,JobID,ReqCPUS,ReqMem,ReqNodes,Constraints,Submit,Eligible,Start,Elapsed,Suspended,End,ExitCode,DerivedExitCode,AllocNodes,NCPUS,MaxDiskRead,MaxDiskWrite,MaxPages,MaxRSS,MaxVMSize,CPUTime,UserCPU,SystemCPU,TotalCPU,Partition --starttime 1970-01-01T0:00:00 > SlurmAccounting.csv","title":"Collecting the data"},{"location":"SlurmLogParser/#parsing-the-job-completion-data","text":"First you must source the setup script to activate the virtual environment and install all the dependencies in it. source setup.sh ./SlurmLogParser.py \\ --sacct-input-file SlurmAccounting.csv \\ --output-csv jobs.csv","title":"Parsing the Job Completion Data"},{"location":"SlurmLogParser/#full-syntax","text":"usage: SlurmLogParser.py [-h] (--show-data-collection-cmd | --sacct-output-file SACCT_OUTPUT_FILE | --sacct-input-file SACCT_INPUT_FILE) [--output-csv OUTPUT_CSV] [--slurm-root SLURM_ROOT] [--starttime STARTTIME] [--endtime ENDTIME] [--debug] Parse Slurm logs. optional arguments: -h, --help show this help message and exit --show-data-collection-cmd Show the command to create SACCT_OUTPUT_FILE. (default: False) --sacct-output-file SACCT_OUTPUT_FILE File where the output of sacct will be written. Cannot be used with --sacct-file. Required if --sacct-input- file not set. (default: None) --sacct-input-file SACCT_INPUT_FILE File with the output of sacct so can process sacct output offline. Cannot be used with --sacct-output- file. Required if --sacct-output-file not set. (default: None) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --slurm-root SLURM_ROOT Directory that contains the Slurm bin directory. (default: None) --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --debug, -d Enable debug mode (default: False)","title":"Full Syntax"},{"location":"SlurmLogParser/#whats-next","text":"Once completed, you can run step #3 (cost simulation) by following the instructions for the JobAnalyzer . Since you already generated a CSV file, you will use JobAnalyzer with the csv option.","title":"What's next?"},{"location":"UnderstandingCSVformat/","text":"Understanding the CSV format The CSV files generated by parsing LSF / Slurm / Accelerator job completion records all have the same structure. This simplifies the cost simulation, as only 1 input type is processed. You can also generate this CSV file yourself from any other scheduler and use JobAnalyzer.py to parse that, but you need to adhere to the structure below (The order of the fields in the CSV file do not matter, but the names of the fields do). CSV structure There is no standard for CSV files, but the JobAnalyzer.py expects the files to be written in the \"Microsoft Excel\" dialect, adn field names must be in the first row. Required Fields Field Type Description job_id string Job id num_cores int Total number of cores allocated to the job max_mem_gb float Total amount of memory allocated to the job in GB num_hosts int Number of hosts. In Slurm this is the number of nodes. The number of cores should be evenly divisible by the number of hosts. For LSF this is typically 1. submit_time datetime string Time that the job was submitted. All times are expected to be in the format YYYY - MM - DD T hh : mm : ss start_time datetime string Time that the job started. finish_time datetime string Time that the job finished. Optional Fields Field Type Description resource_request string Resources requested by the job. For LSF this is the effective resource request. For Slurm it is the job constraints. ineligible_pend_time timedelta string Duration that the job was ineligible to run. This is what LSF writes. Defaults to max(0, eligible_time - start_time). Format for timedelta strings is h : mm : ss . eligible_time datetime string Time that the job became eligible to run. Defaults to the start_time + ineligible_pend_time. requeue_time datetime string Time that the job was requeued. Currently not used. wait_time timedelta string Time that job was pending after it was eligible to run. Default start_time - eligible_time. run_time timedelta string Time that the job ran. Default: finish_time - start_time exit_status int Effective return code of the job. Default: 0 ru_majflt float Number of page faults ru_maxrss float Maximum shared text size ru_minflt float Number of page reclaims ru_msgsnd float Number of System V IPC messages sent ru_msgrcv float Number of messages received ru_nswap float Number of times the process was swapped out ru_inblock float Number of block input operations ru_oublock float Number of block output operations ru_stime float System time used ru_utime float User time used","title":"Understanding the CSV format"},{"location":"UnderstandingCSVformat/#understanding-the-csv-format","text":"The CSV files generated by parsing LSF / Slurm / Accelerator job completion records all have the same structure. This simplifies the cost simulation, as only 1 input type is processed. You can also generate this CSV file yourself from any other scheduler and use JobAnalyzer.py to parse that, but you need to adhere to the structure below (The order of the fields in the CSV file do not matter, but the names of the fields do).","title":"Understanding the CSV format"},{"location":"UnderstandingCSVformat/#csv-structure","text":"There is no standard for CSV files, but the JobAnalyzer.py expects the files to be written in the \"Microsoft Excel\" dialect, adn field names must be in the first row.","title":"CSV structure"},{"location":"UnderstandingCSVformat/#required-fields","text":"Field Type Description job_id string Job id num_cores int Total number of cores allocated to the job max_mem_gb float Total amount of memory allocated to the job in GB num_hosts int Number of hosts. In Slurm this is the number of nodes. The number of cores should be evenly divisible by the number of hosts. For LSF this is typically 1. submit_time datetime string Time that the job was submitted. All times are expected to be in the format YYYY - MM - DD T hh : mm : ss start_time datetime string Time that the job started. finish_time datetime string Time that the job finished.","title":"Required Fields"},{"location":"UnderstandingCSVformat/#optional-fields","text":"Field Type Description resource_request string Resources requested by the job. For LSF this is the effective resource request. For Slurm it is the job constraints. ineligible_pend_time timedelta string Duration that the job was ineligible to run. This is what LSF writes. Defaults to max(0, eligible_time - start_time). Format for timedelta strings is h : mm : ss . eligible_time datetime string Time that the job became eligible to run. Defaults to the start_time + ineligible_pend_time. requeue_time datetime string Time that the job was requeued. Currently not used. wait_time timedelta string Time that job was pending after it was eligible to run. Default start_time - eligible_time. run_time timedelta string Time that the job ran. Default: finish_time - start_time exit_status int Effective return code of the job. Default: 0 ru_majflt float Number of page faults ru_maxrss float Maximum shared text size ru_minflt float Number of page reclaims ru_msgsnd float Number of System V IPC messages sent ru_msgrcv float Number of messages received ru_nswap float Number of times the process was swapped out ru_inblock float Number of block input operations ru_oublock float Number of block output operations ru_stime float System time used ru_utime float User time used","title":"Optional Fields"},{"location":"UpdateInstanceDatabase/","text":"Updating the Instance Type Information While we update the HCS Instance Type Information with each new release, you can update it yourself to get updated instance types and prices for one region or all regions. The Instance Type Information is stored in JSON format in instance_type_info.json Note : Running the script without specifying a region will collect the data for all instance types across all available regions, which may take a few minutes to complete. Prerequisites To be able to update the EC2 Instance Type Information you need: AWS CLI installed AWS CLI Profile with the permissions shown below Required AWS permissions { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"GetEC2InstanceTypeInfo\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:DescribeInstanceTypes\", \"ec2:DescribeRegions\", \"ec2:DescribeSpotPriceHistory\", \"pricing:GetProducts\" ], \"Resource\": \"*\" } ] } Running the update Once your source setup.sh to setup the virtual environment, you can run get_instance_type_info.py to update the Instance Database: source setup.sh rm instance_type_info.json ./get_ec2_instance_info.py --input instance_type_info.json --region eu-west-1 Note: If the script fails to complete for all regions, you can restart it without losing the data already collected by rerunning it with the --input parameter and providing the partial output from the previous run ( instance_type_info.json ) Full Syntax: usage: get_ec2_instance_info.py [-h] [--region REGION] [--input INPUT] [--output-csv OUTPUT_CSV] [--debug] Get EC2 instance pricing info. optional arguments: -h, --help show this help message and exit --region REGION, -r REGION AWS region(s) to get info for. (default: []) --input INPUT, -i INPUT JSON input file. Reads existing info from previous runs. Can speed up rerun if a region failed. (default: None) --output-csv OUTPUT_CSV, -o OUTPUT_CSV CSV output file. Default: instance_type_info.csv (default: None) --debug, -d Enable debug messages (default: False)","title":"Updating the Instance Type Information"},{"location":"UpdateInstanceDatabase/#updating-the-instance-type-information","text":"While we update the HCS Instance Type Information with each new release, you can update it yourself to get updated instance types and prices for one region or all regions. The Instance Type Information is stored in JSON format in instance_type_info.json Note : Running the script without specifying a region will collect the data for all instance types across all available regions, which may take a few minutes to complete.","title":"Updating the Instance Type Information"},{"location":"UpdateInstanceDatabase/#prerequisites","text":"To be able to update the EC2 Instance Type Information you need: AWS CLI installed AWS CLI Profile with the permissions shown below","title":"Prerequisites"},{"location":"UpdateInstanceDatabase/#required-aws-permissions","text":"{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"GetEC2InstanceTypeInfo\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:DescribeInstanceTypes\", \"ec2:DescribeRegions\", \"ec2:DescribeSpotPriceHistory\", \"pricing:GetProducts\" ], \"Resource\": \"*\" } ] }","title":"Required AWS permissions"},{"location":"UpdateInstanceDatabase/#running-the-update","text":"Once your source setup.sh to setup the virtual environment, you can run get_instance_type_info.py to update the Instance Database: source setup.sh rm instance_type_info.json ./get_ec2_instance_info.py --input instance_type_info.json --region eu-west-1 Note: If the script fails to complete for all regions, you can restart it without losing the data already collected by rerunning it with the --input parameter and providing the partial output from the previous run ( instance_type_info.json )","title":"Running the update"},{"location":"UpdateInstanceDatabase/#full-syntax","text":"usage: get_ec2_instance_info.py [-h] [--region REGION] [--input INPUT] [--output-csv OUTPUT_CSV] [--debug] Get EC2 instance pricing info. optional arguments: -h, --help show this help message and exit --region REGION, -r REGION AWS region(s) to get info for. (default: []) --input INPUT, -i INPUT JSON input file. Reads existing info from previous runs. Can speed up rerun if a region failed. (default: None) --output-csv OUTPUT_CSV, -o OUTPUT_CSV CSV output file. Default: instance_type_info.csv (default: None) --debug, -d Enable debug messages (default: False)","title":"Full Syntax:"},{"location":"config/","text":"Configuration All configuration parameters are stored in config.yml . The schema of the configuration file is contained in config_schema.yml and contains a list of all the options. See comments within the files for details of each parameter's use. Key configuration parameters that you may want to change include. Parameter Default Description instance_mapping: region_name eu-west-1 The region where the AWS instances will run. Since instance availability and prices vary by region, this has direct impact on costs instance_mapping: instance_prefix_list [c6i, m5., r5., c5., z1d, x2i] Instance types to be used during the analysis consumption_model_mapping: maximum_minutes_for_spot 60 Threshold for using on-demand instances instead of spot instances. consumption_model_mapping: ec2_savings_plan_duration 3 Duration of the savings plan. Valid values: [1, 3] consumption_model_mapping: ec2_savings_plan_payment_option 'All Upfront' Payment option. Valid values: ['All Upfront', 'Partial Upfront', 'No Upfront'] consumption_model_mapping: 3 Duration of the savings plan. Valid values: [1, 3] consumption_model_mapping: 'All Upfront' Payment option. Valid values: ['All Upfront', 'Partial Upfront', 'No Upfront']","title":"Config"},{"location":"config/#configuration","text":"All configuration parameters are stored in config.yml . The schema of the configuration file is contained in config_schema.yml and contains a list of all the options. See comments within the files for details of each parameter's use. Key configuration parameters that you may want to change include. Parameter Default Description instance_mapping: region_name eu-west-1 The region where the AWS instances will run. Since instance availability and prices vary by region, this has direct impact on costs instance_mapping: instance_prefix_list [c6i, m5., r5., c5., z1d, x2i] Instance types to be used during the analysis consumption_model_mapping: maximum_minutes_for_spot 60 Threshold for using on-demand instances instead of spot instances. consumption_model_mapping: ec2_savings_plan_duration 3 Duration of the savings plan. Valid values: [1, 3] consumption_model_mapping: ec2_savings_plan_payment_option 'All Upfront' Payment option. Valid values: ['All Upfront', 'Partial Upfront', 'No Upfront'] consumption_model_mapping: 3 Duration of the savings plan. Valid values: [1, 3] consumption_model_mapping: 'All Upfront' Payment option. Valid values: ['All Upfront', 'Partial Upfront', 'No Upfront']","title":"Configuration"}]}