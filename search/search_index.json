{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"HPC Cost Simulator This tool analyzes the jobs that have been run on compute clusters managed by the IBM LSF, SchedMD Slurm, and Altair Accelerator schedulers and simulates the hourly cost of running the jobs on AWS. The AWS Cloud Economics team can use this data to build a commercial proposal to optimize the cost using Saving Plans and Reserved Instances. Since scheduler data can indicate business sensitive data (tape out dates, project names) and PII (user names), this tool anonymizes all aspects of the data to meet the security and privacy requirements of Semiconductor companies. All output files are clear-text CSV files for auditability. The tool consists of two steps. The first is the parsing of the scheduler job completion data and is scheduler specific. The output of the parser is a CSV file with the job data that is required for cost analysis. The second step is processing the job data and simulating the cost of running them on AWS. Dependencies Python 3.6 or newer sudo access to install any required tools A bash setup script is provided to install the required yum packages, create a Python virtual environment with all required Python packages, and activate the virtual environment. The pip packages and versions are listed in requirements.txt . The setup script must be sourced, not executed, to set up the environment to run the tool. source setup.sh Instance Type Information The tool requires a list of available instance types in the region and their prices that is stored in instance_type_info.json . The version that comes with the tool has the data for all regions that was complete when it was generated. This is provided for convenience so that the customer can run the tool without an AWS account or IAM permissions. NOTE : Since available instance types and prices change over time this file should be updated before before being used. The get_instance_type_info.py script can be used to update instance_type_info.json . It requires the following IAM permissions. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"GetEC2InstanceTypeInfo\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:DescribeInstanceTypes\", \"ec2:DescribeRegions\", \"ec2:DescribeSpotPriceHistory\", \"pricing:GetProducts\" ], \"Resource\": \"*\" } ] } To get information just for one region run the following commands. source setup.sh rm instance_type_info.json get_ec2_instance_info.py --input instance_type_info.json --region REGION You can get the information for all regions by leaving off the --region argument. source setup.sh rm instance_type_info.json get_ec2_instance_info.py --input instance_type_info.json Running the Tool Job Parsing The scheduler parsers parse the scheduler-specific job completion data and saves it to a common CSV format. The parsers are scheduler specific Python scripts. AcceleratorLogParser.py LSFLogParser.py SlurmLogParser.py Parsing can take several hours to complete and we recommend testing on a small subset of the accounting records before running on the entire dataset. Job Analysis After the job completion data has been parsed, the job analyzer analyzes the CSV file, simulates the costs on AWS, and builds the output files. summary.csv summary_stats.csv hourly_stats.csv hourly_stats.xlsx The job analyzer can call the parser, however, parsing is the longest step so it is recommended that it be run separately from the job analyzer. The parser output is a CSV file that can be used as the input to the job analyzer. The job analyzer runs relatively quickly and can be run with different configurations on the same job data. summary.csv Provides aggregated data on the jobs (at the individual task level) including the number of jobs, the total runtime the total wait time. These provide a high-level understanding of the customer's workload, and helps us show the customer how much of their workload needs high memory instances (utually only a small percentage) and therefore the opportunity for savings from right-sizing their compute instaces. Example: MemorySize 0-1 Minutes <-- <-- 1-5 Minutes <-- <-- 5-20 Minutes <-- <-- 20-60 Minutes <-- <-- 60-240 Minutes <-- <-- 240-1440 Minutes <-- <-- 1440-1000000 Min <-- <-- Job count Total duration Total wait time Job count Total duration Total wait time Job count Total duration Total wait time Job count Total duration Total wait time Job count Total duration Total wait time Job count Total duration <-- Job count Total duration Total wait time 0-1GB 1451224 295357 0 298597 711710 0 268244 2464401 0 84075 2832260 0 48673 5699891 0 21450 14928398 <-- 37222 940366200 0 1-2GB 7800 4645 0 33076 88381 0 27230 285958 0 11301 390683 0 9244 1129832 0 5195 3431642 <-- 4444 104405917 0 2-4GB 3866 2482 0 68157 201630 0 94798 1120678 0 108126 3764462 0 47593 5155126 0 17470 10450704 <-- 7154 100846783 0 4-8GB 425 219 0 5352 17497 0 9978 106535 0 7959 275142 0 5768 701616 0 3831 2662791 <-- 3218 66155544 0 8-16GB 759 274 0 722 2044 0 5904 72149 0 4462 183369 0 11244 1307581 0 6363 3983032 <-- 3024 42831455 0 16-32GB 389 108 0 237 771 0 1148 14321 0 1573 58971 0 2971 359024 0 2577 1766540 <-- 2017 29588185 0 32-64GB 435 133 0 63 209 0 605 7613 0 1079 41389 0 1731 206706 0 1283 1026257 <-- 1107 22509888 0 64-128GB 160 60 0 71 140 0 35 383 0 110 4379 0 244 33840 0 408 284120 <-- 285 6939359 0 128-256GB 177 81 0 125 275 0 107 743 0 45 1826 0 122 15285 0 258 206859 <-- 192 5132956 0 256-512GB 580 227 0 297 889 0 330 3258 0 249 9267 0 165 20148 0 53 35973 <-- 69 1955220 0 512-1000000GB 498 249 0 447 918 0 226 2116 0 141 4907 0 45 5168 0 41 29869 <-- 26 729017 0 In this (Real custoemr) example, over 95% of the jobs were not memory intensive (below 32GB/Core), but the customer is sizing their entire HPC fleet for the more memory intensive 5% of the workload. hourly_stats.csv Provides an hour-by-hour cost simulation broken down by spot and on-demand costs. Relative Hour Total OnDemand Costs Total Spot Cost m5 r5 0 8.93 0.00 8.93 0.00 1 10.97 0.00 10.97 0.00 2 11.02 0.00 11.02 0.00 3 10.88 0.00 10.88 0.00 4 11.02 0.00 11.02 0.00 5 10.96 0.00 10.96 0.00 6 11.00 0.00 11.00 0.00 7 11.02 0.00 11.02 0.00 8 11.03 0.00 11.03 0.00 9 11.22 0.00 11.16 0.05 10 12.93 0.00 11.26 1.66 11 12.98 0.00 11.54 1.44 12 13.04 0.00 11.21 1.82 13 10.25 0.00 10.21 0.03 14 11.92 0.00 11.07 0.84 15 15.58 0.00 11.53 4.05 16 13.97 0.00 12.09 1.87 17 17.04 0.00 12.82 4.21 18 25.28 0.00 12.23 13.05","title":"HPC Cost Simulator"},{"location":"#hpc-cost-simulator","text":"This tool analyzes the jobs that have been run on compute clusters managed by the IBM LSF, SchedMD Slurm, and Altair Accelerator schedulers and simulates the hourly cost of running the jobs on AWS. The AWS Cloud Economics team can use this data to build a commercial proposal to optimize the cost using Saving Plans and Reserved Instances. Since scheduler data can indicate business sensitive data (tape out dates, project names) and PII (user names), this tool anonymizes all aspects of the data to meet the security and privacy requirements of Semiconductor companies. All output files are clear-text CSV files for auditability. The tool consists of two steps. The first is the parsing of the scheduler job completion data and is scheduler specific. The output of the parser is a CSV file with the job data that is required for cost analysis. The second step is processing the job data and simulating the cost of running them on AWS.","title":"HPC Cost Simulator"},{"location":"#dependencies","text":"Python 3.6 or newer sudo access to install any required tools A bash setup script is provided to install the required yum packages, create a Python virtual environment with all required Python packages, and activate the virtual environment. The pip packages and versions are listed in requirements.txt . The setup script must be sourced, not executed, to set up the environment to run the tool. source setup.sh","title":"Dependencies"},{"location":"#instance-type-information","text":"The tool requires a list of available instance types in the region and their prices that is stored in instance_type_info.json . The version that comes with the tool has the data for all regions that was complete when it was generated. This is provided for convenience so that the customer can run the tool without an AWS account or IAM permissions. NOTE : Since available instance types and prices change over time this file should be updated before before being used. The get_instance_type_info.py script can be used to update instance_type_info.json . It requires the following IAM permissions. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"GetEC2InstanceTypeInfo\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:DescribeInstanceTypes\", \"ec2:DescribeRegions\", \"ec2:DescribeSpotPriceHistory\", \"pricing:GetProducts\" ], \"Resource\": \"*\" } ] } To get information just for one region run the following commands. source setup.sh rm instance_type_info.json get_ec2_instance_info.py --input instance_type_info.json --region REGION You can get the information for all regions by leaving off the --region argument. source setup.sh rm instance_type_info.json get_ec2_instance_info.py --input instance_type_info.json","title":"Instance Type Information"},{"location":"#running-the-tool","text":"","title":"Running the Tool"},{"location":"#job-parsing","text":"The scheduler parsers parse the scheduler-specific job completion data and saves it to a common CSV format. The parsers are scheduler specific Python scripts. AcceleratorLogParser.py LSFLogParser.py SlurmLogParser.py Parsing can take several hours to complete and we recommend testing on a small subset of the accounting records before running on the entire dataset.","title":"Job Parsing"},{"location":"#job-analysis","text":"After the job completion data has been parsed, the job analyzer analyzes the CSV file, simulates the costs on AWS, and builds the output files. summary.csv summary_stats.csv hourly_stats.csv hourly_stats.xlsx The job analyzer can call the parser, however, parsing is the longest step so it is recommended that it be run separately from the job analyzer. The parser output is a CSV file that can be used as the input to the job analyzer. The job analyzer runs relatively quickly and can be run with different configurations on the same job data.","title":"Job Analysis"},{"location":"#summarycsv","text":"Provides aggregated data on the jobs (at the individual task level) including the number of jobs, the total runtime the total wait time. These provide a high-level understanding of the customer's workload, and helps us show the customer how much of their workload needs high memory instances (utually only a small percentage) and therefore the opportunity for savings from right-sizing their compute instaces. Example: MemorySize 0-1 Minutes <-- <-- 1-5 Minutes <-- <-- 5-20 Minutes <-- <-- 20-60 Minutes <-- <-- 60-240 Minutes <-- <-- 240-1440 Minutes <-- <-- 1440-1000000 Min <-- <-- Job count Total duration Total wait time Job count Total duration Total wait time Job count Total duration Total wait time Job count Total duration Total wait time Job count Total duration Total wait time Job count Total duration <-- Job count Total duration Total wait time 0-1GB 1451224 295357 0 298597 711710 0 268244 2464401 0 84075 2832260 0 48673 5699891 0 21450 14928398 <-- 37222 940366200 0 1-2GB 7800 4645 0 33076 88381 0 27230 285958 0 11301 390683 0 9244 1129832 0 5195 3431642 <-- 4444 104405917 0 2-4GB 3866 2482 0 68157 201630 0 94798 1120678 0 108126 3764462 0 47593 5155126 0 17470 10450704 <-- 7154 100846783 0 4-8GB 425 219 0 5352 17497 0 9978 106535 0 7959 275142 0 5768 701616 0 3831 2662791 <-- 3218 66155544 0 8-16GB 759 274 0 722 2044 0 5904 72149 0 4462 183369 0 11244 1307581 0 6363 3983032 <-- 3024 42831455 0 16-32GB 389 108 0 237 771 0 1148 14321 0 1573 58971 0 2971 359024 0 2577 1766540 <-- 2017 29588185 0 32-64GB 435 133 0 63 209 0 605 7613 0 1079 41389 0 1731 206706 0 1283 1026257 <-- 1107 22509888 0 64-128GB 160 60 0 71 140 0 35 383 0 110 4379 0 244 33840 0 408 284120 <-- 285 6939359 0 128-256GB 177 81 0 125 275 0 107 743 0 45 1826 0 122 15285 0 258 206859 <-- 192 5132956 0 256-512GB 580 227 0 297 889 0 330 3258 0 249 9267 0 165 20148 0 53 35973 <-- 69 1955220 0 512-1000000GB 498 249 0 447 918 0 226 2116 0 141 4907 0 45 5168 0 41 29869 <-- 26 729017 0 In this (Real custoemr) example, over 95% of the jobs were not memory intensive (below 32GB/Core), but the customer is sizing their entire HPC fleet for the more memory intensive 5% of the workload.","title":"summary.csv"},{"location":"#hourly_statscsv","text":"Provides an hour-by-hour cost simulation broken down by spot and on-demand costs. Relative Hour Total OnDemand Costs Total Spot Cost m5 r5 0 8.93 0.00 8.93 0.00 1 10.97 0.00 10.97 0.00 2 11.02 0.00 11.02 0.00 3 10.88 0.00 10.88 0.00 4 11.02 0.00 11.02 0.00 5 10.96 0.00 10.96 0.00 6 11.00 0.00 11.00 0.00 7 11.02 0.00 11.02 0.00 8 11.03 0.00 11.03 0.00 9 11.22 0.00 11.16 0.05 10 12.93 0.00 11.26 1.66 11 12.98 0.00 11.54 1.44 12 13.04 0.00 11.21 1.82 13 10.25 0.00 10.21 0.03 14 11.92 0.00 11.07 0.84 15 15.58 0.00 11.53 4.05 16 13.97 0.00 12.09 1.87 17 17.04 0.00 12.82 4.21 18 25.28 0.00 12.23 13.05","title":"hourly_stats.csv"},{"location":"AcceleratorLogParser/","text":"Altair Accelerator Log Parsing The AcceleratorLogParser.py script queries the Accelerator results database, parses the job completion data, and saves it into a CSV file. Usage usage: AcceleratorLogParser.py [-h] (--sql-output-file SQL_OUTPUT_FILE | --sql-input-file SQL_INPUT_FILE) --output-csv OUTPUT_CSV [--debug] Parse Altair Accelerator logs. optional arguments: -h, --help show this help message and exit --sql-output-file SQL_OUTPUT_FILE File where the output of sql query will be written. Cannot be used with --sql-input-file. Required if --sql-input-file not set. Command to create file: nc cmd vovsql_query -e \"select id, submittime, starttime, endtime, exitstatus, maxram, maxvm, cputime, susptime from jobs\" > SQL_OUTPUT_FILE (default: None) --sql-input-file SQL_INPUT_FILE File with the output of sql query so can process it offline. Cannot be used with --sql-output-file. Required if --sql-output-file not set. (default: None) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --debug, -d Enable debug mode (default: False) Parsing the Job Completion Data First you must source the setup script to make sure that all required packages are installed. source setup.sh This creates a python virtual environment and activates it. The script calls nc cmd vovsql_query so your environment should be set up so that the nc scripts are in the path. For example: source /tools/altr/2019.01u7/common/etc/vovrc.sh The parsed data will be written in the output directory which will be created if it does not exist. ./AcceleratorLogParser.py \\ --sql-output-file output/sql-output.txt \\ --output-csv output/jobs.csv","title":"Altair Accelerator Log Parsing"},{"location":"AcceleratorLogParser/#altair-accelerator-log-parsing","text":"The AcceleratorLogParser.py script queries the Accelerator results database, parses the job completion data, and saves it into a CSV file.","title":"Altair Accelerator Log Parsing"},{"location":"AcceleratorLogParser/#usage","text":"usage: AcceleratorLogParser.py [-h] (--sql-output-file SQL_OUTPUT_FILE | --sql-input-file SQL_INPUT_FILE) --output-csv OUTPUT_CSV [--debug] Parse Altair Accelerator logs. optional arguments: -h, --help show this help message and exit --sql-output-file SQL_OUTPUT_FILE File where the output of sql query will be written. Cannot be used with --sql-input-file. Required if --sql-input-file not set. Command to create file: nc cmd vovsql_query -e \"select id, submittime, starttime, endtime, exitstatus, maxram, maxvm, cputime, susptime from jobs\" > SQL_OUTPUT_FILE (default: None) --sql-input-file SQL_INPUT_FILE File with the output of sql query so can process it offline. Cannot be used with --sql-output-file. Required if --sql-output-file not set. (default: None) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --debug, -d Enable debug mode (default: False)","title":"Usage"},{"location":"AcceleratorLogParser/#parsing-the-job-completion-data","text":"First you must source the setup script to make sure that all required packages are installed. source setup.sh This creates a python virtual environment and activates it. The script calls nc cmd vovsql_query so your environment should be set up so that the nc scripts are in the path. For example: source /tools/altr/2019.01u7/common/etc/vovrc.sh The parsed data will be written in the output directory which will be created if it does not exist. ./AcceleratorLogParser.py \\ --sql-output-file output/sql-output.txt \\ --output-csv output/jobs.csv","title":"Parsing the Job Completion Data"},{"location":"JobAnalyzer/","text":"Job Analyzer The JobAnalyzer.py script post-processes the output of the parsers to provide an analysis of running the jobs on AWS. For convenience, the analyzer can call the parser and analyze the output in 1 step. Prerequisites JobAnalyzer.py relies on instance_type_info.json which contains instance type details and pricing. If the file doesn't exist in the same directory as the script then it will attempt to create it using the AWS API. For details on the required IAM permissions see the main documentation page . Configuration All configuration parameters are stored in config.yml . The schema of the configuration file is contained in config_schema.yml and contains a list of all the options. See comments within the files for details of each parameter's use. Key configuration parameters that you may want to change include. Parameter Default Description instance_mapping: region_name eu-west-1 The region where the AWS instances will run. instance_mapping: instance_prefix_list [c6i, m5., r5., c5., z1d, x2i] Instance types to be used during the analysis consumption_model_mapping: maximum_minutes_for_spot 60 Threshold for using on-demand instances instead of spot instances. consumption_model_mapping: ec2_savings_plan_duration 3 Duration of the savings plan. Valid values: [1, 3] consumption_model_mapping: ec2_savings_plan_payment_option 'All Upfront' Payment option. Valid values: ['All Upfront', 'Partial Upfront', 'No Upfront'] consumption_model_mapping: 3 Duration of the savings plan. Valid values: [1, 3] consumption_model_mapping: 'All Upfront' Payment option. Valid values: ['All Upfront', 'Partial Upfront', 'No Upfront'] Usage Arguments provided to JobAnalyzer are required in a specific order: ./JobAnalyzer.py <Arguments that apply to all schedulers> <Parser type> <Parser-specific Arguments> These are the common arguments. usage: JobAnalyzer.py [-h] [--starttime STARTTIME] [--endtime ENDTIME] [--config CONFIG] [--output-dir OUTPUT_DIR] [--output-csv OUTPUT_CSV] [--debug] parser ... Analyze jobs positional arguments: parser Choose the kind of information to parse. ./JobAnalyzer.py <parser> -h for parser specific arguments. accelerator Parse Accelerator (nc) job information csv Parse CSV from already parsed job information. lsf Parse LSF logfiles slurm Parse Slurm job information hourly_stats Parse hourly_stats file so can create Excel workbook (xlsx). optional arguments: -h, --help show this help message and exit --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --config CONFIG Configuration file. (default: ./config.yml) --output-dir OUTPUT_DIR Directory where output will be written (default: output) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --debug, -d Enable debug mode (default: False) Arguments that apply to all schedulers When used, these parameters must precede the parser type: --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM-DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM-DDTHH:MM:SS (default: None) --config CONFIG Configuration file. (default: ./config.yml) --output-dir OUTPUT_DIR Directory where output will be written (default: output) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --debug, -d Enable debug mode (default: False) --help, -h Show help message Parser Type The tool supports 4 parser types: accelerator Parse Accelerator (nc) job information lsf Parse LSF accounting ercords (lsb.acct fiels) slurm Parse Slurm job information csv Parse CSV from a previously parsed job information. Parser-specific Arguments - Accelerator usage: JobAnalyzer.py accelerator [-h] [--default-mem-gb DEFAULT_MEM_GB] (--sql-output-file SQL_OUTPUT_FILE | --sql-input-file SQL_INPUT_FILE) optional arguments: -h, --help show this help message and exit --default-mem-gb DEFAULT_MEM_GB Default amount of memory (in GB) requested for jobs. (default: 0.098) --sql-output-file SQL_OUTPUT_FILE File where the output of sql query will be written. Cannot be used with --sql-input-file. Required if --sql-input-file not set. Command to create file: nc cmd vovsql_query -e \"select jobs.id, jobs.submittime, jobs.starttime, jobs.endtime, resources.name, jobs.exitstatus, jobs.maxram, jobs.maxvm, jobs.cputime, jobs.susptime from jobs inner join resources on jobs.resourcesid=resources.id\" > SQL_OUTPUT_FILE (default: None) --sql-input-file SQL_INPUT_FILE File with the output of sql query so can process it offline. Cannot be used with --sql-output-file. Required if --sql-output-file not set. (default: None) Parser-specific Arguments - LSF usage: JobAnalyzer.py lsf [-h] [--logfile-dir LOGFILE_DIR] optional arguments: -h, --help show this help message and exit --logfile-dir LOGFILE_DIR LSF logfile directory (default: None) Parser-specific Arguments - Slurm usage: JobAnalyzer.py slurm [-h] [--slurm-root SLURM_ROOT] [--sacct-output-file SACCT_OUTPUT_FILE | --sacct-input-file SACCT_INPUT_FILE] optional arguments: -h, --help show this help message and exit --slurm-root SLURM_ROOT Directory that contains the Slurm bin directory. (default: None) --sacct-output-file SACCT_OUTPUT_FILE File where the output of sacct will be written. Cannot be used with --sacct-input-file. Required if --sacct- input-file not set. (default: None) --sacct-input-file SACCT_INPUT_FILE File with the output of sacct so can process sacct output offline. Cannot be used with --sacct-output- file. Required if --sacct-output-file not set. (default: None) Note: The tool will call sacct to get accounting logs, if you don't have it installed on the machine, please see the SlurmLogParser.py documentation for details on how to save them to a CSV file. Analyzing Pre-Parsed CSV Job Data usage: JobAnalyzer.py csv [-h] --input-csv INPUT_CSV optional arguments: -h, --help show this help message and exit --input-csv INPUT_CSV CSV file with parsed job info from scheduler parser. (default: None) Data Used The parser parses out and save the minimum amount of data required to do the analysis. The fields are documented in the SchedulerJobInfo.py module. The intent is only to store timestamp information, the number of cores and amount of memory requested by each job, and actual resource usage information, if available such as the user time, system time, and max memory used.","title":"Job Analyzer"},{"location":"JobAnalyzer/#job-analyzer","text":"The JobAnalyzer.py script post-processes the output of the parsers to provide an analysis of running the jobs on AWS. For convenience, the analyzer can call the parser and analyze the output in 1 step.","title":"Job Analyzer"},{"location":"JobAnalyzer/#prerequisites","text":"JobAnalyzer.py relies on instance_type_info.json which contains instance type details and pricing. If the file doesn't exist in the same directory as the script then it will attempt to create it using the AWS API. For details on the required IAM permissions see the main documentation page .","title":"Prerequisites"},{"location":"JobAnalyzer/#configuration","text":"All configuration parameters are stored in config.yml . The schema of the configuration file is contained in config_schema.yml and contains a list of all the options. See comments within the files for details of each parameter's use. Key configuration parameters that you may want to change include. Parameter Default Description instance_mapping: region_name eu-west-1 The region where the AWS instances will run. instance_mapping: instance_prefix_list [c6i, m5., r5., c5., z1d, x2i] Instance types to be used during the analysis consumption_model_mapping: maximum_minutes_for_spot 60 Threshold for using on-demand instances instead of spot instances. consumption_model_mapping: ec2_savings_plan_duration 3 Duration of the savings plan. Valid values: [1, 3] consumption_model_mapping: ec2_savings_plan_payment_option 'All Upfront' Payment option. Valid values: ['All Upfront', 'Partial Upfront', 'No Upfront'] consumption_model_mapping: 3 Duration of the savings plan. Valid values: [1, 3] consumption_model_mapping: 'All Upfront' Payment option. Valid values: ['All Upfront', 'Partial Upfront', 'No Upfront']","title":"Configuration"},{"location":"JobAnalyzer/#usage","text":"Arguments provided to JobAnalyzer are required in a specific order: ./JobAnalyzer.py <Arguments that apply to all schedulers> <Parser type> <Parser-specific Arguments> These are the common arguments. usage: JobAnalyzer.py [-h] [--starttime STARTTIME] [--endtime ENDTIME] [--config CONFIG] [--output-dir OUTPUT_DIR] [--output-csv OUTPUT_CSV] [--debug] parser ... Analyze jobs positional arguments: parser Choose the kind of information to parse. ./JobAnalyzer.py <parser> -h for parser specific arguments. accelerator Parse Accelerator (nc) job information csv Parse CSV from already parsed job information. lsf Parse LSF logfiles slurm Parse Slurm job information hourly_stats Parse hourly_stats file so can create Excel workbook (xlsx). optional arguments: -h, --help show this help message and exit --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --config CONFIG Configuration file. (default: ./config.yml) --output-dir OUTPUT_DIR Directory where output will be written (default: output) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --debug, -d Enable debug mode (default: False)","title":"Usage"},{"location":"JobAnalyzer/#arguments-that-apply-to-all-schedulers","text":"When used, these parameters must precede the parser type: --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM-DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM-DDTHH:MM:SS (default: None) --config CONFIG Configuration file. (default: ./config.yml) --output-dir OUTPUT_DIR Directory where output will be written (default: output) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --debug, -d Enable debug mode (default: False) --help, -h Show help message","title":"Arguments that apply to all schedulers"},{"location":"JobAnalyzer/#parser-type","text":"The tool supports 4 parser types: accelerator Parse Accelerator (nc) job information lsf Parse LSF accounting ercords (lsb.acct fiels) slurm Parse Slurm job information csv Parse CSV from a previously parsed job information.","title":"Parser Type"},{"location":"JobAnalyzer/#parser-specific-arguments-accelerator","text":"usage: JobAnalyzer.py accelerator [-h] [--default-mem-gb DEFAULT_MEM_GB] (--sql-output-file SQL_OUTPUT_FILE | --sql-input-file SQL_INPUT_FILE) optional arguments: -h, --help show this help message and exit --default-mem-gb DEFAULT_MEM_GB Default amount of memory (in GB) requested for jobs. (default: 0.098) --sql-output-file SQL_OUTPUT_FILE File where the output of sql query will be written. Cannot be used with --sql-input-file. Required if --sql-input-file not set. Command to create file: nc cmd vovsql_query -e \"select jobs.id, jobs.submittime, jobs.starttime, jobs.endtime, resources.name, jobs.exitstatus, jobs.maxram, jobs.maxvm, jobs.cputime, jobs.susptime from jobs inner join resources on jobs.resourcesid=resources.id\" > SQL_OUTPUT_FILE (default: None) --sql-input-file SQL_INPUT_FILE File with the output of sql query so can process it offline. Cannot be used with --sql-output-file. Required if --sql-output-file not set. (default: None)","title":"Parser-specific Arguments - Accelerator"},{"location":"JobAnalyzer/#parser-specific-arguments-lsf","text":"usage: JobAnalyzer.py lsf [-h] [--logfile-dir LOGFILE_DIR] optional arguments: -h, --help show this help message and exit --logfile-dir LOGFILE_DIR LSF logfile directory (default: None)","title":"Parser-specific Arguments - LSF"},{"location":"JobAnalyzer/#parser-specific-arguments-slurm","text":"usage: JobAnalyzer.py slurm [-h] [--slurm-root SLURM_ROOT] [--sacct-output-file SACCT_OUTPUT_FILE | --sacct-input-file SACCT_INPUT_FILE] optional arguments: -h, --help show this help message and exit --slurm-root SLURM_ROOT Directory that contains the Slurm bin directory. (default: None) --sacct-output-file SACCT_OUTPUT_FILE File where the output of sacct will be written. Cannot be used with --sacct-input-file. Required if --sacct- input-file not set. (default: None) --sacct-input-file SACCT_INPUT_FILE File with the output of sacct so can process sacct output offline. Cannot be used with --sacct-output- file. Required if --sacct-output-file not set. (default: None) Note: The tool will call sacct to get accounting logs, if you don't have it installed on the machine, please see the SlurmLogParser.py documentation for details on how to save them to a CSV file.","title":"Parser-specific Arguments - Slurm"},{"location":"JobAnalyzer/#analyzing-pre-parsed-csv-job-data","text":"usage: JobAnalyzer.py csv [-h] --input-csv INPUT_CSV optional arguments: -h, --help show this help message and exit --input-csv INPUT_CSV CSV file with parsed job info from scheduler parser. (default: None)","title":"Analyzing Pre-Parsed CSV Job Data"},{"location":"JobAnalyzer/#data-used","text":"The parser parses out and save the minimum amount of data required to do the analysis. The fields are documented in the SchedulerJobInfo.py module. The intent is only to store timestamp information, the number of cores and amount of memory requested by each job, and actual resource usage information, if available such as the user time, system time, and max memory used.","title":"Data Used"},{"location":"LSFLogParser/","text":"IBM Spectrum LSF Parser The LSFLogParser.py script parses the lsb.acct* files in the LSF logfile directory and writes the parsed data into a CSV file that can be read by JobAnalyzer.py . Usage usage: LSFLogParser.py [-h] --logfile-dir LOGFILE_DIR --output-csv OUTPUT_CSV --default-max-mem-gb DEFAULT_MAX_MEM_GB [--starttime STARTTIME] [--endtime ENDTIME] [--debug] Parse LSF logs. optional arguments: -h, --help show this help message and exit --logfile-dir LOGFILE_DIR LSF logfile directory (default: None) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --default-max-mem-gb DEFAULT_MAX_MEM_GB Default maximum memory for a job in GB. (default: None) --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --debug, -d Enable debug mode (default: False) Parsing the Log Files First you must source the setup script to make sure that all required packages are installed. source setup.sh This creates a python virtual environment and activates it. The script will parse all of the files that start with lsb.acct* in the logfile directory. The parsed data will be written in the output directory which will be created if it does not exist. The parsed job data will be written out in csv format. ./LSFLogParser.py \\ --logfile-dir <logfile-dir> \\ --output-csv output/jobs.csv","title":"IBM Spectrum LSF Parser"},{"location":"LSFLogParser/#ibm-spectrum-lsf-parser","text":"The LSFLogParser.py script parses the lsb.acct* files in the LSF logfile directory and writes the parsed data into a CSV file that can be read by JobAnalyzer.py .","title":"IBM Spectrum LSF Parser"},{"location":"LSFLogParser/#usage","text":"usage: LSFLogParser.py [-h] --logfile-dir LOGFILE_DIR --output-csv OUTPUT_CSV --default-max-mem-gb DEFAULT_MAX_MEM_GB [--starttime STARTTIME] [--endtime ENDTIME] [--debug] Parse LSF logs. optional arguments: -h, --help show this help message and exit --logfile-dir LOGFILE_DIR LSF logfile directory (default: None) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --default-max-mem-gb DEFAULT_MAX_MEM_GB Default maximum memory for a job in GB. (default: None) --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --debug, -d Enable debug mode (default: False)","title":"Usage"},{"location":"LSFLogParser/#parsing-the-log-files","text":"First you must source the setup script to make sure that all required packages are installed. source setup.sh This creates a python virtual environment and activates it. The script will parse all of the files that start with lsb.acct* in the logfile directory. The parsed data will be written in the output directory which will be created if it does not exist. The parsed job data will be written out in csv format. ./LSFLogParser.py \\ --logfile-dir <logfile-dir> \\ --output-csv output/jobs.csv","title":"Parsing the Log Files"},{"location":"SlurmLogParser/","text":"Slurm Log Parser The SlurmLogParser.py script calls Slurm sacct , parses the job completion data, and generates a CSV file that can be used by JobAnalyzer.py . It has 2 modes of operation, online and offline. Online Online means the tool will call the sacct command directly, which requires the Linux machine running the tool to have sacct installed and configured as well as the permissions to access accounting records from all users. When running in this mode, the output of sacct will be stored to a CSV file defined by the --sacct-output-file parameter. Offline If running this tool on a Linux machine not connected to the Slurm head-node, you can first extract the sacct output to a CSV file and then run the tool offline to analyze it. When running in this mode, the tool expects an existing file, defined by the --sacct-input-file argument, that contains the output of saact . Note : You can't use both --sacct-input-file and --sacct-output-file together. Usage usage: SlurmLogParser.py [-h] [--slurm-root SLURM_ROOT] (--sacct-output-file SACCT_OUTPUT_FILE | --sacct-input-file SACCT_INPUT_FILE) --output-csv OUTPUT_CSV [--debug] Parse Slurm logs. optional arguments: -h, --help show this help message and exit --slurm-root SLURM_ROOT Directory that contains the Slurm bin directory. (default: None) --sacct-output-file SACCT_OUTPUT_FILE File where the output of sacct will be written. Cannot be used with --sacct-file. Required if --sacct-input- file not set. (default: None) --sacct-input-file SACCT_INPUT_FILE File with the output of sacct so can process sacct output offline. Cannot be used with --sacct-output- file. Required if --sacct-output-file not set. (default: None) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --debug, -d Enable debug mode (default: False) Parsing the Job Completion Data First you must source the setup script to make sure that all required packages are installed. source setup.sh This creates a python virtual environment and activates it. The script calls sacct so your environment should be set up so that the Slurm scripts are in the path. Otherwise you can pass the path to the scripts usingn the --slurm-root argument which should point to the path that contains the Slurm bin directory. The parsed data will be written in the output directory which will be created if it does not exist. ./SlurmLogParser.py \\ --sacct-output-file output/sacct-output.txt \\ --output-csv output/jobs.csv","title":"Slurm Log Parser"},{"location":"SlurmLogParser/#slurm-log-parser","text":"The SlurmLogParser.py script calls Slurm sacct , parses the job completion data, and generates a CSV file that can be used by JobAnalyzer.py . It has 2 modes of operation, online and offline.","title":"Slurm Log Parser"},{"location":"SlurmLogParser/#online","text":"Online means the tool will call the sacct command directly, which requires the Linux machine running the tool to have sacct installed and configured as well as the permissions to access accounting records from all users. When running in this mode, the output of sacct will be stored to a CSV file defined by the --sacct-output-file parameter.","title":"Online"},{"location":"SlurmLogParser/#offline","text":"If running this tool on a Linux machine not connected to the Slurm head-node, you can first extract the sacct output to a CSV file and then run the tool offline to analyze it. When running in this mode, the tool expects an existing file, defined by the --sacct-input-file argument, that contains the output of saact . Note : You can't use both --sacct-input-file and --sacct-output-file together.","title":"Offline"},{"location":"SlurmLogParser/#usage","text":"usage: SlurmLogParser.py [-h] [--slurm-root SLURM_ROOT] (--sacct-output-file SACCT_OUTPUT_FILE | --sacct-input-file SACCT_INPUT_FILE) --output-csv OUTPUT_CSV [--debug] Parse Slurm logs. optional arguments: -h, --help show this help message and exit --slurm-root SLURM_ROOT Directory that contains the Slurm bin directory. (default: None) --sacct-output-file SACCT_OUTPUT_FILE File where the output of sacct will be written. Cannot be used with --sacct-file. Required if --sacct-input- file not set. (default: None) --sacct-input-file SACCT_INPUT_FILE File with the output of sacct so can process sacct output offline. Cannot be used with --sacct-output- file. Required if --sacct-output-file not set. (default: None) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --debug, -d Enable debug mode (default: False)","title":"Usage"},{"location":"SlurmLogParser/#parsing-the-job-completion-data","text":"First you must source the setup script to make sure that all required packages are installed. source setup.sh This creates a python virtual environment and activates it. The script calls sacct so your environment should be set up so that the Slurm scripts are in the path. Otherwise you can pass the path to the scripts usingn the --slurm-root argument which should point to the path that contains the Slurm bin directory. The parsed data will be written in the output directory which will be created if it does not exist. ./SlurmLogParser.py \\ --sacct-output-file output/sacct-output.txt \\ --output-csv output/jobs.csv","title":"Parsing the Job Completion Data"},{"location":"todo/","text":"Things To Do Manage memory consumption. Don't require all of the jobs to be stored in memory so can handle a virtually unlimited number of jobs. Add Altair Accelerator support","title":"Things To Do"},{"location":"todo/#things-to-do","text":"Manage memory consumption. Don't require all of the jobs to be stored in memory so can handle a virtually unlimited number of jobs. Add Altair Accelerator support","title":"Things To Do"}]}