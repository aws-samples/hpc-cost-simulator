{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"HPC Cost Simulator (HCS) HPC Cost Simulator (HCS) enables customers to simulate their High Performance Computing (HPC) costs on AWS. To simulate your future costs, HCS analyzes past job records (scheduler accounting database) and performs hour-by-hour cost analysis. Supported schedulers: IBM LSF SchedMD Slurm Altair Accelerator The output allows customers to: Estimate their Amazon Elastic Compute Cloud (Amazon EC2) costs hour-by-hour Choose the optimal consumption model to minimize costs, including Savings Plans , Reserved Instances and Spot Instances Get statistics about their workloads, locating specific job configurations that are waiting in queue longer, reducing engineering productivity. Data privacy HCS avoids the need to share scheduler logs with AWS. Scheduler logs include business sensitive data (tape out dates, project names) and PII (user names). Instead, customers run HCS on premises, creating an anonymized output that may be shared with your AWS account team to get additional cost optimization guidance. The output file (located under the output/ directory if not otherwise specified) is a simple Excel (.xslx), allowing you to audit its contents before sharing it. How data is analyzed The analysis is a 4-step process: # Step Description 1 Data Collection Collect accounting records from the database. 2 Formatting Parsing the scheduler accounting database - this takes the scheduler specific format, and converts the job records to a uniform format (not scheduler specific). 3 Cost Simulation Performing cost simulation using the output from step 2. 4 Analysis Analyzing the output, building a complete cost model. This can be done using your AWS account team. Prerequisites Python 3.6 or newer sudo access to install any required tools Recommended: CentOS 7 or higher A bash setup script is provided to install the required packages and create a Python virtual environment. The pip packages and versions are listed in requirements.txt . The setup script must be sourced, not executed, to set up the environment to run the tool. source setup.sh Configuration HCS comes pre-configured with a few best practices for analyzing an Electronics Design Automation (EDA) workload. You can edit the configuration by editing config.yml before running HCS. We encourage you to read the file before running to tool, to know what parameters are used to simulate the workload. A partial list of these configuration parameters includes: AWS region Instance selection guidelines Savings Plans selection guidelines Spot selection guidelines Instance Type Information To perform cost simulation, HCS requires a list of available instance types in the region and their prices that is stored in instance_type_info.json . HCS comes pre-populated with the database, and we periodically update it to reflect new instance types, prices etc. However, to update it yourself , see Updating Instance Type Information Running the Tool Steps 1 & 2 These steps are scheduler-specific and are explained in each scheduler's documentation page: IBM LSF SchedMD Slurm Altair Engineering Accelerator If you want to parse data from an unsupported scheduler, you can export the accounting data from that scheduler generating your own CSV, as long as you CSV structure defined in Understanding the CSV format Step 3 Once completed, you can run step #3 (cost simulation) by following the instructions in for the JobAnalyzer Notes for step #3 The job analyzer can call the parser (step 2), however, parsing is the longest step so it is recommended that you run it separately. The job analyzer can be run with different configuration file ( config.yml ) on the same job data. For example: Simulating your workload costs with and without Spot Instances Simulating your costs with different instance families Step 4 For analysis, you can either share the Excel file from step #3 with your AWS account team, or read the Output page for more details.","title":"HPC Cost Simulator (HCS)"},{"location":"#hpc-cost-simulator-hcs","text":"HPC Cost Simulator (HCS) enables customers to simulate their High Performance Computing (HPC) costs on AWS. To simulate your future costs, HCS analyzes past job records (scheduler accounting database) and performs hour-by-hour cost analysis. Supported schedulers: IBM LSF SchedMD Slurm Altair Accelerator The output allows customers to: Estimate their Amazon Elastic Compute Cloud (Amazon EC2) costs hour-by-hour Choose the optimal consumption model to minimize costs, including Savings Plans , Reserved Instances and Spot Instances Get statistics about their workloads, locating specific job configurations that are waiting in queue longer, reducing engineering productivity.","title":"HPC Cost Simulator (HCS)"},{"location":"#data-privacy","text":"HCS avoids the need to share scheduler logs with AWS. Scheduler logs include business sensitive data (tape out dates, project names) and PII (user names). Instead, customers run HCS on premises, creating an anonymized output that may be shared with your AWS account team to get additional cost optimization guidance. The output file (located under the output/ directory if not otherwise specified) is a simple Excel (.xslx), allowing you to audit its contents before sharing it.","title":"Data privacy"},{"location":"#how-data-is-analyzed","text":"The analysis is a 4-step process: # Step Description 1 Data Collection Collect accounting records from the database. 2 Formatting Parsing the scheduler accounting database - this takes the scheduler specific format, and converts the job records to a uniform format (not scheduler specific). 3 Cost Simulation Performing cost simulation using the output from step 2. 4 Analysis Analyzing the output, building a complete cost model. This can be done using your AWS account team.","title":"How data is analyzed"},{"location":"#prerequisites","text":"Python 3.6 or newer sudo access to install any required tools Recommended: CentOS 7 or higher A bash setup script is provided to install the required packages and create a Python virtual environment. The pip packages and versions are listed in requirements.txt . The setup script must be sourced, not executed, to set up the environment to run the tool. source setup.sh","title":"Prerequisites"},{"location":"#configuration","text":"HCS comes pre-configured with a few best practices for analyzing an Electronics Design Automation (EDA) workload. You can edit the configuration by editing config.yml before running HCS. We encourage you to read the file before running to tool, to know what parameters are used to simulate the workload. A partial list of these configuration parameters includes: AWS region Instance selection guidelines Savings Plans selection guidelines Spot selection guidelines","title":"Configuration"},{"location":"#instance-type-information","text":"To perform cost simulation, HCS requires a list of available instance types in the region and their prices that is stored in instance_type_info.json . HCS comes pre-populated with the database, and we periodically update it to reflect new instance types, prices etc. However, to update it yourself , see Updating Instance Type Information","title":"Instance Type Information"},{"location":"#running-the-tool","text":"","title":"Running the Tool"},{"location":"#steps-1-2","text":"These steps are scheduler-specific and are explained in each scheduler's documentation page: IBM LSF SchedMD Slurm Altair Engineering Accelerator If you want to parse data from an unsupported scheduler, you can export the accounting data from that scheduler generating your own CSV, as long as you CSV structure defined in Understanding the CSV format","title":"Steps 1 &amp; 2"},{"location":"#step-3","text":"Once completed, you can run step #3 (cost simulation) by following the instructions in for the JobAnalyzer Notes for step #3 The job analyzer can call the parser (step 2), however, parsing is the longest step so it is recommended that you run it separately. The job analyzer can be run with different configuration file ( config.yml ) on the same job data. For example: Simulating your workload costs with and without Spot Instances Simulating your costs with different instance families","title":"Step 3"},{"location":"#step-4","text":"For analysis, you can either share the Excel file from step #3 with your AWS account team, or read the Output page for more details.","title":"Step 4"},{"location":"AcceleratorLogParser/","text":"Altair Accelerator The AcceleratorLogParser.py script queries the Accelerator results database, parses the job completion data, and saves it into a CSV file. Video walkthrough The fastest way to learn about using HPC Cost Simulator with Altair's Accelerator (NC) is to watch this short walkthrough. Collecting the data To see data collection command syntax, run: source setup.sh ./AcceleratorLogParser.py --show-data-collection-cmd For example, as of this writing, the command is: nc cmd vovsql_query -e \"select jobs.id, jobs.submittime, jobs.starttime, jobs.endtime, resources.name, jobs.exitstatus, jobs.maxram, jobs.maxvm, jobs.cputime, jobs.susptime from jobs inner join resources on jobs.resourcesid=resources.id\" > sql-output.txt This command should run on a node connected to the Altair Accelerator head node, and with a user with permissions to see the jobs from all users. Note: To be able to run the command, you first need to source the vovrc.sh script located in your /common/etc subfolder of your Altair install folder. Parsing the Job Completion Data The parsed data will be written in the output directory which will be created if it does not exist. ./AcceleratorLogParser.py \\ --sql-output-file sql-output.txt \\ --output-csv output/jobs.csv Full Syntax usage: AcceleratorLogParser.py [-h] (--show-data-collection-cmd | --sql-output-file SQL_OUTPUT_FILE | --sql-input-file SQL_INPUT_FILE) [--output-csv OUTPUT_CSV] [--default-mem-gb DEFAULT_MEM_GB] [--starttime STARTTIME] [--endtime ENDTIME] [--debug] Parse Altair Accelerator logs. optional arguments: -h, --help show this help message and exit --show-data-collection-cmd Show the command to create the SQL_OUTPUT_FILE. (default: False) --sql-output-file SQL_OUTPUT_FILE File where the output of sql query will be written. Cannot be used with --sql-input-file. Required if --sql-input-file not set. (default: None) --sql-input-file SQL_INPUT_FILE File with the output of sql query so can process it offline. Cannot be used with --sql-output-file. Required if --sql-output-file not set. (default: None) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --default-mem-gb DEFAULT_MEM_GB Default amount of memory (in GB) requested for jobs that do not have a memory request. (default: 0.0) --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --debug, -d Enable debug mode (default: False) What's next? Once completed, you can run step #3 (cost simulation) by following the instructions in for the JobAnalyzer Since you already generated a CSV file, you will use JobAnalyzer with the csv option.","title":"Altair Accelerator"},{"location":"AcceleratorLogParser/#altair-accelerator","text":"The AcceleratorLogParser.py script queries the Accelerator results database, parses the job completion data, and saves it into a CSV file.","title":"Altair Accelerator"},{"location":"AcceleratorLogParser/#video-walkthrough","text":"The fastest way to learn about using HPC Cost Simulator with Altair's Accelerator (NC) is to watch this short walkthrough.","title":"Video walkthrough"},{"location":"AcceleratorLogParser/#collecting-the-data","text":"To see data collection command syntax, run: source setup.sh ./AcceleratorLogParser.py --show-data-collection-cmd For example, as of this writing, the command is: nc cmd vovsql_query -e \"select jobs.id, jobs.submittime, jobs.starttime, jobs.endtime, resources.name, jobs.exitstatus, jobs.maxram, jobs.maxvm, jobs.cputime, jobs.susptime from jobs inner join resources on jobs.resourcesid=resources.id\" > sql-output.txt This command should run on a node connected to the Altair Accelerator head node, and with a user with permissions to see the jobs from all users. Note: To be able to run the command, you first need to source the vovrc.sh script located in your /common/etc subfolder of your Altair install folder.","title":"Collecting the data"},{"location":"AcceleratorLogParser/#parsing-the-job-completion-data","text":"The parsed data will be written in the output directory which will be created if it does not exist. ./AcceleratorLogParser.py \\ --sql-output-file sql-output.txt \\ --output-csv output/jobs.csv","title":"Parsing the Job Completion Data"},{"location":"AcceleratorLogParser/#full-syntax","text":"usage: AcceleratorLogParser.py [-h] (--show-data-collection-cmd | --sql-output-file SQL_OUTPUT_FILE | --sql-input-file SQL_INPUT_FILE) [--output-csv OUTPUT_CSV] [--default-mem-gb DEFAULT_MEM_GB] [--starttime STARTTIME] [--endtime ENDTIME] [--debug] Parse Altair Accelerator logs. optional arguments: -h, --help show this help message and exit --show-data-collection-cmd Show the command to create the SQL_OUTPUT_FILE. (default: False) --sql-output-file SQL_OUTPUT_FILE File where the output of sql query will be written. Cannot be used with --sql-input-file. Required if --sql-input-file not set. (default: None) --sql-input-file SQL_INPUT_FILE File with the output of sql query so can process it offline. Cannot be used with --sql-output-file. Required if --sql-output-file not set. (default: None) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --default-mem-gb DEFAULT_MEM_GB Default amount of memory (in GB) requested for jobs that do not have a memory request. (default: 0.0) --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --debug, -d Enable debug mode (default: False)","title":"Full Syntax"},{"location":"AcceleratorLogParser/#whats-next","text":"Once completed, you can run step #3 (cost simulation) by following the instructions in for the JobAnalyzer Since you already generated a CSV file, you will use JobAnalyzer with the csv option.","title":"What's next?"},{"location":"JobAnalyzer/","text":"Job Analyzer The JobAnalyzer.py performs the cost simulation using the outputs from AcceleratorLogParser.py , LSFLogParser.py or SlurmLogParser.py It produces an hour-by-hour cost simulation, placing the output in the output/ subfolder (by default). For convenience, the analyzer can call the parser and analyze the output in 1 step, however we recommend performing the analysis in separate stages (see \"How data is analyzed\") in index.md . Video walkthrough The fastest way to learn about using JobAnalyzer.py to simulate your HPC Costsis to watch this short walkthrough. Once you've run the tools, you'll see the hourly_stats.xlsx file under your output subdirectory. To learn how to use this file and apply cost optimizaitons, go to the Outputs documenration page and watch the video walkthrough there. Prerequisites JobAnalyzer.py relies on: the virtual environment created by the setup script. run source setup.sh to setup the virtual environment. config.yml which defines the configuration of the analysis. For more details on the configuration file, see the configuration documentation instance_type_info.json which contains instance type details and pricing. The file is part of the repository, but if you want to download an update list of instances and their prices, please see Updating the Instance Type Information Performing the cost simulation Once you generated a CSV file from your Scheduler (See instructions for IBM LSF , SchedMD Slurm abd Altair Engineering Accelerator ) you can perform the cost simulation (step 3) using JobAnalyzer.py To parse the CSV file into the final anonymized Excel report, run: source setup.sh ./JobAnalyzer.py csv --input-csv INPUT_CSV_FILE_NAME Outputs By default, HCS places all output files in the output/ folder (this can be changed using the --output-dir parameter to JobAnalyzer.py ). Note: The output folder will get overwritten without prompting you for approval. Full Syntax Arguments provided to JobAnalyzer are required in a specific order: ./JobAnalyzer.py <Arguments that apply to all schedulers> <Parser type> <Parser-specific Arguments> These are the common arguments. usage: JobAnalyzer.py [-h] [--starttime STARTTIME] [--endtime ENDTIME] [--config CONFIG] [--acknowledge-config] [--output-dir OUTPUT_DIR] [--output-csv OUTPUT_CSV] [--debug] parser ... Analyze jobs positional arguments: parser Choose the kind of information to parse. ./JobAnalyzer.py <parser> -h for parser specific arguments. accelerator Parse Accelerator (nc) job information csv Parse CSV from already parsed job information. lsf Parse LSF logfiles slurm Parse Slurm job information hourly_stats Parse hourly_stats file so can create Excel workbook (xlsx). optional arguments: -h, --help show this help message and exit --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --config CONFIG Configuration file. (default: ./config.yml) --acknowledge-config Acknowledge configuration file contents so don't get prompt. (default: False) --output-dir OUTPUT_DIR Directory where output will be written (default: output) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --debug, -d Enable debug mode (default: False) Arguments that apply to all schedulers When used, these parameters must precede the parser type: --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM-DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM-DDTHH:MM:SS (default: None) --config CONFIG Configuration file. (default: ./config.yml) --output-dir OUTPUT_DIR Directory where output will be written (default: output) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --debug, -d Enable debug mode (default: False) --help, -h Show help message Parser Type The tool supports 5 parser types: accelerator Parse Accelerator (nc) job information lsf Parse LSF accounting records (lsb.acct fields) slurm Parse Slurm job information csv Parse CSV from a previously parsed job information. hourly_stats Parse the hourly output files from a previous run Parser-Specific Arguments - Accelerator usage: JobAnalyzer.py accelerator [-h] [--default-mem-gb DEFAULT_MEM_GB] (--sql-output-file SQL_OUTPUT_FILE | --sql-input-file SQL_INPUT_FILE) optional arguments: -h, --help show this help message and exit --default-mem-gb DEFAULT_MEM_GB Default amount of memory (in GB) requested for jobs. (default: 0.0) --sql-output-file SQL_OUTPUT_FILE File where the output of sql query will be written. Cannot be used with --sql-input-file. Required if --sql-input-file not set. Command to create file: nc cmd vovsql_query -e \"select jobs.id, jobs.submittime, jobs.starttime, jobs.endtime, resources.name, jobs.exitstatus, jobs.maxram, jobs.maxvm, jobs.cputime, jobs.susptime from jobs inner join resources on jobs.resourcesid=resources.id\" > SQL_OUTPUT_FILE (default: None) --sql-input-file SQL_INPUT_FILE File with the output of sql query so can process it offline. Cannot be used with --sql-output-file. Required if --sql-output-file not set. (default: None) Parser-Specific Arguments - LSF usage: JobAnalyzer.py lsf [-h] [--logfile-dir LOGFILE_DIR] --default-max-mem-gb DEFAULT_MAX_MEM_GB optional arguments: -h, --help show this help message and exit --logfile-dir LOGFILE_DIR LSF logfile directory (default: None) --default-max-mem-gb DEFAULT_MAX_MEM_GB Default maximum memory for a job in GB. (default: None) Parser-Specific Arguments - Slurm usage: JobAnalyzer.py slurm [-h] [--slurm-root SLURM_ROOT] [--sacct-output-file SACCT_OUTPUT_FILE | --sacct-input-file SACCT_INPUT_FILE] optional arguments: -h, --help show this help message and exit --slurm-root SLURM_ROOT Directory that contains the Slurm bin directory. (default: None) --sacct-output-file SACCT_OUTPUT_FILE File where the output of sacct will be written. Cannot be used with --sacct-input-file. Required if --sacct- input-file not set. (default: None) --sacct-input-file SACCT_INPUT_FILE File with the output of sacct so can process sacct output offline. Cannot be used with --sacct-output- file. Required if --sacct-output-file not set. (default: None) Note: The tool will call sacct to get accounting logs, if you don't have it installed on the machine, please see the SlurmLogParser.py documentation for details on how to save them to a CSV file. What's Next? Once completed, you can find your Excel report under output/hourly_stats.xlsx . You can share it with your AWS account team (Recommended) for further cost optimization guidance, or you can learn more about the data in the Excel in the Output documentation page.","title":"Job Analyzer"},{"location":"JobAnalyzer/#job-analyzer","text":"The JobAnalyzer.py performs the cost simulation using the outputs from AcceleratorLogParser.py , LSFLogParser.py or SlurmLogParser.py It produces an hour-by-hour cost simulation, placing the output in the output/ subfolder (by default). For convenience, the analyzer can call the parser and analyze the output in 1 step, however we recommend performing the analysis in separate stages (see \"How data is analyzed\") in index.md .","title":"Job Analyzer"},{"location":"JobAnalyzer/#video-walkthrough","text":"The fastest way to learn about using JobAnalyzer.py to simulate your HPC Costsis to watch this short walkthrough. Once you've run the tools, you'll see the hourly_stats.xlsx file under your output subdirectory. To learn how to use this file and apply cost optimizaitons, go to the Outputs documenration page and watch the video walkthrough there.","title":"Video walkthrough"},{"location":"JobAnalyzer/#prerequisites","text":"JobAnalyzer.py relies on: the virtual environment created by the setup script. run source setup.sh to setup the virtual environment. config.yml which defines the configuration of the analysis. For more details on the configuration file, see the configuration documentation instance_type_info.json which contains instance type details and pricing. The file is part of the repository, but if you want to download an update list of instances and their prices, please see Updating the Instance Type Information","title":"Prerequisites"},{"location":"JobAnalyzer/#performing-the-cost-simulation","text":"Once you generated a CSV file from your Scheduler (See instructions for IBM LSF , SchedMD Slurm abd Altair Engineering Accelerator ) you can perform the cost simulation (step 3) using JobAnalyzer.py To parse the CSV file into the final anonymized Excel report, run: source setup.sh ./JobAnalyzer.py csv --input-csv INPUT_CSV_FILE_NAME","title":"Performing the cost simulation"},{"location":"JobAnalyzer/#outputs","text":"By default, HCS places all output files in the output/ folder (this can be changed using the --output-dir parameter to JobAnalyzer.py ). Note: The output folder will get overwritten without prompting you for approval.","title":"Outputs"},{"location":"JobAnalyzer/#full-syntax","text":"Arguments provided to JobAnalyzer are required in a specific order: ./JobAnalyzer.py <Arguments that apply to all schedulers> <Parser type> <Parser-specific Arguments> These are the common arguments. usage: JobAnalyzer.py [-h] [--starttime STARTTIME] [--endtime ENDTIME] [--config CONFIG] [--acknowledge-config] [--output-dir OUTPUT_DIR] [--output-csv OUTPUT_CSV] [--debug] parser ... Analyze jobs positional arguments: parser Choose the kind of information to parse. ./JobAnalyzer.py <parser> -h for parser specific arguments. accelerator Parse Accelerator (nc) job information csv Parse CSV from already parsed job information. lsf Parse LSF logfiles slurm Parse Slurm job information hourly_stats Parse hourly_stats file so can create Excel workbook (xlsx). optional arguments: -h, --help show this help message and exit --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --config CONFIG Configuration file. (default: ./config.yml) --acknowledge-config Acknowledge configuration file contents so don't get prompt. (default: False) --output-dir OUTPUT_DIR Directory where output will be written (default: output) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --debug, -d Enable debug mode (default: False)","title":"Full Syntax"},{"location":"JobAnalyzer/#arguments-that-apply-to-all-schedulers","text":"When used, these parameters must precede the parser type: --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM-DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM-DDTHH:MM:SS (default: None) --config CONFIG Configuration file. (default: ./config.yml) --output-dir OUTPUT_DIR Directory where output will be written (default: output) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --debug, -d Enable debug mode (default: False) --help, -h Show help message","title":"Arguments that apply to all schedulers"},{"location":"JobAnalyzer/#parser-type","text":"The tool supports 5 parser types: accelerator Parse Accelerator (nc) job information lsf Parse LSF accounting records (lsb.acct fields) slurm Parse Slurm job information csv Parse CSV from a previously parsed job information. hourly_stats Parse the hourly output files from a previous run","title":"Parser Type"},{"location":"JobAnalyzer/#parser-specific-arguments-accelerator","text":"usage: JobAnalyzer.py accelerator [-h] [--default-mem-gb DEFAULT_MEM_GB] (--sql-output-file SQL_OUTPUT_FILE | --sql-input-file SQL_INPUT_FILE) optional arguments: -h, --help show this help message and exit --default-mem-gb DEFAULT_MEM_GB Default amount of memory (in GB) requested for jobs. (default: 0.0) --sql-output-file SQL_OUTPUT_FILE File where the output of sql query will be written. Cannot be used with --sql-input-file. Required if --sql-input-file not set. Command to create file: nc cmd vovsql_query -e \"select jobs.id, jobs.submittime, jobs.starttime, jobs.endtime, resources.name, jobs.exitstatus, jobs.maxram, jobs.maxvm, jobs.cputime, jobs.susptime from jobs inner join resources on jobs.resourcesid=resources.id\" > SQL_OUTPUT_FILE (default: None) --sql-input-file SQL_INPUT_FILE File with the output of sql query so can process it offline. Cannot be used with --sql-output-file. Required if --sql-output-file not set. (default: None)","title":"Parser-Specific Arguments - Accelerator"},{"location":"JobAnalyzer/#parser-specific-arguments-lsf","text":"usage: JobAnalyzer.py lsf [-h] [--logfile-dir LOGFILE_DIR] --default-max-mem-gb DEFAULT_MAX_MEM_GB optional arguments: -h, --help show this help message and exit --logfile-dir LOGFILE_DIR LSF logfile directory (default: None) --default-max-mem-gb DEFAULT_MAX_MEM_GB Default maximum memory for a job in GB. (default: None)","title":"Parser-Specific Arguments - LSF"},{"location":"JobAnalyzer/#parser-specific-arguments-slurm","text":"usage: JobAnalyzer.py slurm [-h] [--slurm-root SLURM_ROOT] [--sacct-output-file SACCT_OUTPUT_FILE | --sacct-input-file SACCT_INPUT_FILE] optional arguments: -h, --help show this help message and exit --slurm-root SLURM_ROOT Directory that contains the Slurm bin directory. (default: None) --sacct-output-file SACCT_OUTPUT_FILE File where the output of sacct will be written. Cannot be used with --sacct-input-file. Required if --sacct- input-file not set. (default: None) --sacct-input-file SACCT_INPUT_FILE File with the output of sacct so can process sacct output offline. Cannot be used with --sacct-output- file. Required if --sacct-output-file not set. (default: None) Note: The tool will call sacct to get accounting logs, if you don't have it installed on the machine, please see the SlurmLogParser.py documentation for details on how to save them to a CSV file.","title":"Parser-Specific Arguments - Slurm"},{"location":"JobAnalyzer/#whats-next","text":"Once completed, you can find your Excel report under output/hourly_stats.xlsx . You can share it with your AWS account team (Recommended) for further cost optimization guidance, or you can learn more about the data in the Excel in the Output documentation page.","title":"What's Next?"},{"location":"JsonLogParser/","text":"JsonLogParser The JsonLogParser.py script parses job information from a JSON file that contains an array of dicts that contain the required job information. This allows you to run the Job Analyzer on jobs from an unsupported scheduler as long as you can export the job information in the required JSON format. The parser also requires you to provide a schema mapping file that maps the field names in your json file to the field names used by the job analyzer. JSON Jobs File Format The expected JSON format looks like the following example. [ { \"Job_Id\": \"1\", \"ncpus\": \"1\", \"mem_bytes\": 13843545600, \"node_count\": 1, \"ctime\": \"2024-06-28T01:00:00\", \"stime\": \"2024-06-28T01:00:00\", \"walltime\": \"2024-06-28T01:00:00\", \"etime\": \"2024-06-28T01:00:00\", \"queue\": \"regress\", \"project\": \"project1\", \"Exit_status\": \"0\", \"ru_mem_bytes\": 7334404096 }, { \"Job_Id\": \"2\", \"ncpus\": \"1\", \"mem_bytes\": 13843545600, \"node_count\": 1, \"ctime\": \"2024-06-28T01:00:00\", \"stime\": \"2024-06-28T01:00:00\", \"walltime\": \"2024-06-28T01:00:00\", \"etime\": \"2024-06-28T01:00:00\", \"queue\": \"regress\", \"project\": \"project1\", \"Exit_status\": \"0\", \"ru_mem_bytes\": 7334404096 } ] The field names aren't fixed, but must be the same for each job record. JSON Schema Map File Format The schema map is a JSON file that maps the standard field names used by the parser to field names of your JSON file. It looks like the following and must contain the following required fields. For numeric fields like max_mem_gb, you can specify the units and the number will be scaled. For example if you log contains memory request in bytes, then you can specify that and the number will be converted to GB. \"job_id\": \"Job_Id\", \"num_cores\": \"ncpus\", \"max_mem_gb\": {\"mem_bytes\": {\"units\": \"b\"}}, \"num_hosts\": \"node_count\", \"submit_time\": \"ctime\", \"start_time\": \"stime\", \"run_time\": \"walltime\", \"eligible_time\": \"etime\", \"queue\": \"queue\", \"project\": \"project\", \"exit_status\": \"Exit_status\", \"ru_maxrss\": \"ru_mem_bytes\" Parsing the accounting JSON log files First you must source the setup script to make sure that all required packages are installed. source setup.sh This creates a python virtual environment and activates it. The script will parse a JSON file that contains the job information. The parsed data will be written in the output directory which will be created if it does not exist. The parsed job data will be written out in csv format. ./JsonLogParser.py \\ --input-json <json-jobs-file> \\ --json-schema <json-schema-file> --output-csv jobs.csv Full Syntax usage: JsonLogParser.py [-h] --input-json INPUT_JSON --json-schema-map JSON_SCHEMA_MAP [--output-csv OUTPUT_CSV] [--starttime STARTTIME] [--endtime ENDTIME] [--disable-version-check] [--debug] Parse JSON file with job results. optional arguments: -h, --help show this help message and exit --input-json INPUT_JSON Json file with parsed job info. (default: None) --json-schema-map JSON_SCHEMA_MAP Json file that maps input json field names to SchedulerJobInfo field names. (default: None) --output-csv OUTPUT_CSV CSV file where parsed jobs will be written. (default: None) --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --disable-version-check Disable git version check (default: False) --debug, -d Enable debug mode (default: False) What's next? Once completed, you can run step #3 (cost simulation) by following the instructions in for the JobAnalyzer Since you already generated a CSV file, you will use JobAnalyzer with the csv option.","title":"JsonLogParser"},{"location":"JsonLogParser/#jsonlogparser","text":"The JsonLogParser.py script parses job information from a JSON file that contains an array of dicts that contain the required job information. This allows you to run the Job Analyzer on jobs from an unsupported scheduler as long as you can export the job information in the required JSON format. The parser also requires you to provide a schema mapping file that maps the field names in your json file to the field names used by the job analyzer.","title":"JsonLogParser"},{"location":"JsonLogParser/#json-jobs-file-format","text":"The expected JSON format looks like the following example. [ { \"Job_Id\": \"1\", \"ncpus\": \"1\", \"mem_bytes\": 13843545600, \"node_count\": 1, \"ctime\": \"2024-06-28T01:00:00\", \"stime\": \"2024-06-28T01:00:00\", \"walltime\": \"2024-06-28T01:00:00\", \"etime\": \"2024-06-28T01:00:00\", \"queue\": \"regress\", \"project\": \"project1\", \"Exit_status\": \"0\", \"ru_mem_bytes\": 7334404096 }, { \"Job_Id\": \"2\", \"ncpus\": \"1\", \"mem_bytes\": 13843545600, \"node_count\": 1, \"ctime\": \"2024-06-28T01:00:00\", \"stime\": \"2024-06-28T01:00:00\", \"walltime\": \"2024-06-28T01:00:00\", \"etime\": \"2024-06-28T01:00:00\", \"queue\": \"regress\", \"project\": \"project1\", \"Exit_status\": \"0\", \"ru_mem_bytes\": 7334404096 } ] The field names aren't fixed, but must be the same for each job record.","title":"JSON Jobs File Format"},{"location":"JsonLogParser/#json-schema-map-file-format","text":"The schema map is a JSON file that maps the standard field names used by the parser to field names of your JSON file. It looks like the following and must contain the following required fields. For numeric fields like max_mem_gb, you can specify the units and the number will be scaled. For example if you log contains memory request in bytes, then you can specify that and the number will be converted to GB. \"job_id\": \"Job_Id\", \"num_cores\": \"ncpus\", \"max_mem_gb\": {\"mem_bytes\": {\"units\": \"b\"}}, \"num_hosts\": \"node_count\", \"submit_time\": \"ctime\", \"start_time\": \"stime\", \"run_time\": \"walltime\", \"eligible_time\": \"etime\", \"queue\": \"queue\", \"project\": \"project\", \"exit_status\": \"Exit_status\", \"ru_maxrss\": \"ru_mem_bytes\"","title":"JSON Schema Map File Format"},{"location":"JsonLogParser/#parsing-the-accounting-json-log-files","text":"First you must source the setup script to make sure that all required packages are installed. source setup.sh This creates a python virtual environment and activates it. The script will parse a JSON file that contains the job information. The parsed data will be written in the output directory which will be created if it does not exist. The parsed job data will be written out in csv format. ./JsonLogParser.py \\ --input-json <json-jobs-file> \\ --json-schema <json-schema-file> --output-csv jobs.csv","title":"Parsing the accounting JSON log files"},{"location":"JsonLogParser/#full-syntax","text":"usage: JsonLogParser.py [-h] --input-json INPUT_JSON --json-schema-map JSON_SCHEMA_MAP [--output-csv OUTPUT_CSV] [--starttime STARTTIME] [--endtime ENDTIME] [--disable-version-check] [--debug] Parse JSON file with job results. optional arguments: -h, --help show this help message and exit --input-json INPUT_JSON Json file with parsed job info. (default: None) --json-schema-map JSON_SCHEMA_MAP Json file that maps input json field names to SchedulerJobInfo field names. (default: None) --output-csv OUTPUT_CSV CSV file where parsed jobs will be written. (default: None) --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --disable-version-check Disable git version check (default: False) --debug, -d Enable debug mode (default: False)","title":"Full Syntax"},{"location":"JsonLogParser/#whats-next","text":"Once completed, you can run step #3 (cost simulation) by following the instructions in for the JobAnalyzer Since you already generated a CSV file, you will use JobAnalyzer with the csv option.","title":"What's next?"},{"location":"LSFLogParser/","text":"IBM Spectrum LSF The LSFLogParser.py script parses the lsb.acct* files in the LSF logfile directory and writes the parsed data into a CSV file that can be read by JobAnalyzer.py . Video walkthrough The fastest way to learn about using HPC Cost Simulator with IMB LSF is to watch this short walkthrough. Parsing the accounting log files First you must source the setup script to make sure that all required packages are installed. source setup.sh This creates a python virtual environment and activates it. The script will parse all of the files that start with lsb.acct* in the logfile directory. The parsed data will be written in the output directory which will be created if it does not exist. The parsed job data will be written out in csv format. ./LSFLogParser.py \\ --logfile-dir <logfile-dir> \\ --output-csv jobs.csv Note: The parser expects the file names to start with lsb.acct , do not rename the files. Full Syntax usage: LSFLogParser.py [-h] --logfile-dir LOGFILE_DIR --output-csv OUTPUT_CSV [--default-max-mem-gb DEFAULT_MAX_MEM_GB] [--starttime STARTTIME] [--endtime ENDTIME] [--debug] Parse LSF logs. optional arguments: -h, --help show this help message and exit --logfile-dir LOGFILE_DIR LSF logfile directory (default: None) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --default-max-mem-gb DEFAULT_MAX_MEM_GB Default maximum memory for a job in GB. (default: 0.0) --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --debug, -d Enable debug mode (default: False) What's next? Once completed, you can run step #3 (cost simulation) by following the instructions in for the JobAnalyzer Since you already generated a CSV file, you will use JobAnalyzer with the csv option.","title":"IBM Spectrum LSF"},{"location":"LSFLogParser/#ibm-spectrum-lsf","text":"The LSFLogParser.py script parses the lsb.acct* files in the LSF logfile directory and writes the parsed data into a CSV file that can be read by JobAnalyzer.py .","title":"IBM Spectrum LSF"},{"location":"LSFLogParser/#video-walkthrough","text":"The fastest way to learn about using HPC Cost Simulator with IMB LSF is to watch this short walkthrough.","title":"Video walkthrough"},{"location":"LSFLogParser/#parsing-the-accounting-log-files","text":"First you must source the setup script to make sure that all required packages are installed. source setup.sh This creates a python virtual environment and activates it. The script will parse all of the files that start with lsb.acct* in the logfile directory. The parsed data will be written in the output directory which will be created if it does not exist. The parsed job data will be written out in csv format. ./LSFLogParser.py \\ --logfile-dir <logfile-dir> \\ --output-csv jobs.csv Note: The parser expects the file names to start with lsb.acct , do not rename the files.","title":"Parsing the accounting log files"},{"location":"LSFLogParser/#full-syntax","text":"usage: LSFLogParser.py [-h] --logfile-dir LOGFILE_DIR --output-csv OUTPUT_CSV [--default-max-mem-gb DEFAULT_MAX_MEM_GB] [--starttime STARTTIME] [--endtime ENDTIME] [--debug] Parse LSF logs. optional arguments: -h, --help show this help message and exit --logfile-dir LOGFILE_DIR LSF logfile directory (default: None) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --default-max-mem-gb DEFAULT_MAX_MEM_GB Default maximum memory for a job in GB. (default: 0.0) --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --debug, -d Enable debug mode (default: False)","title":"Full Syntax"},{"location":"LSFLogParser/#whats-next","text":"Once completed, you can run step #3 (cost simulation) by following the instructions in for the JobAnalyzer Since you already generated a CSV file, you will use JobAnalyzer with the csv option.","title":"What's next?"},{"location":"Output/","text":"Output Video walkthrough To learn how to use the hourly_stats.xlsx file and apply cost optimizaitons, watch the video walkthrough there. output files HCS generates multiple output files: File Name Quantity Description hourly_stats.xlsx 1 The full anonymized output, the only file you'll share with your AWS account team (should you choose to do so hourly-######.csv 1 per hour Temporary files, includes all the jobs that started in a specific hour summary.csv 1 Aggregated summary of all your jobs (statistics). This data also appears in the Excel output file hourly_stats.csv 1 Host simulation output, includes. This data also appears in the Excel output file. JobAnalyzer-####-##-##_##-##-##.log 1 Simulation log file for trouble shooting hourly_stats.xlsx This is the main output file, and will be analyzed moving forward. All other files are only used for troubleshooting and or re-running a subset of the analysis. The Excel spreadsheet provides a convenient way to view the data and perform calculations on it. For example, in the following example the \" First hour to analyze \" was changed to only include the last 12 months worth of data. The InstanceFamilySummary worksheet can be used to get insights into the predicted usage of different instance families. In this example it shows that the c6i family is used the most with very little utilization of other instance families. The average hourly use can be used to help choose the upper constraint for the savings plans. Savings Plan Optimization hourly_stats.xlsx allows you to optimize your costs further by simulating the impact of Savings Plans (SP) on your overall costs, and finding the optimal SP cost for your workload. To do this, you will need to enable Excel's numerical solver add-in. To use the solver, you first need to enable it. Windows: Select File -> Options Select Add-ins on the lower left. If the Solver Add-in isn't active then select Manage: Excel Add-ins and click Go . Select the Solver Add-in and click OK . If the Solver Add-in is already active then it will show up like this. Mac OS: Select Tools -> Options If the Solver Add-in isn't active then select it and click OK . Optimize Savings Plans Using Excel Solver Now you are ready to run the solver. Copy the original values by selecting column B and pasting the values into column C (This will allow you to compare before vs. after cost optimization). Then select the Data menu select Solver on the ribbon. Configure the solver by selecting the total cost as the Objective . Then select the Savings Plan commit cells in By Changing Variable Cells . Do not select the total cell, only the $ value for specific instance family types: Optionally, you may add constraints for the Savings Plan commit cells. Windows: Mac OS Then click Solve and wait while the Solver calculates the savings plan commits that will minimize your overall costs. The solver may run a while, but when it finishes then save the results. Windows: Mac OS: The spreadsheet will then show the savings plan commits that minimize the total costs. In this case Savings Plans were able to reduce total costs by about 10%. Note that this is due to the very variable nature of the jobs. The more uniform usage you have, the more cost-effective Savings Plans will be. This graph shows the variability of the workload.","title":"Output"},{"location":"Output/#output","text":"","title":"Output"},{"location":"Output/#video-walkthrough","text":"To learn how to use the hourly_stats.xlsx file and apply cost optimizaitons, watch the video walkthrough there.","title":"Video walkthrough"},{"location":"Output/#output-files","text":"HCS generates multiple output files: File Name Quantity Description hourly_stats.xlsx 1 The full anonymized output, the only file you'll share with your AWS account team (should you choose to do so hourly-######.csv 1 per hour Temporary files, includes all the jobs that started in a specific hour summary.csv 1 Aggregated summary of all your jobs (statistics). This data also appears in the Excel output file hourly_stats.csv 1 Host simulation output, includes. This data also appears in the Excel output file. JobAnalyzer-####-##-##_##-##-##.log 1 Simulation log file for trouble shooting","title":"output files"},{"location":"Output/#hourly_statsxlsx","text":"This is the main output file, and will be analyzed moving forward. All other files are only used for troubleshooting and or re-running a subset of the analysis. The Excel spreadsheet provides a convenient way to view the data and perform calculations on it. For example, in the following example the \" First hour to analyze \" was changed to only include the last 12 months worth of data. The InstanceFamilySummary worksheet can be used to get insights into the predicted usage of different instance families. In this example it shows that the c6i family is used the most with very little utilization of other instance families. The average hourly use can be used to help choose the upper constraint for the savings plans.","title":"hourly_stats.xlsx"},{"location":"Output/#savings-plan-optimization","text":"hourly_stats.xlsx allows you to optimize your costs further by simulating the impact of Savings Plans (SP) on your overall costs, and finding the optimal SP cost for your workload. To do this, you will need to enable Excel's numerical solver add-in. To use the solver, you first need to enable it.","title":"Savings Plan Optimization"},{"location":"Output/#windows","text":"Select File -> Options Select Add-ins on the lower left. If the Solver Add-in isn't active then select Manage: Excel Add-ins and click Go . Select the Solver Add-in and click OK . If the Solver Add-in is already active then it will show up like this.","title":"Windows:"},{"location":"Output/#mac-os","text":"Select Tools -> Options If the Solver Add-in isn't active then select it and click OK .","title":"Mac OS:"},{"location":"Output/#optimize-savings-plans-using-excel-solver","text":"Now you are ready to run the solver. Copy the original values by selecting column B and pasting the values into column C (This will allow you to compare before vs. after cost optimization). Then select the Data menu select Solver on the ribbon. Configure the solver by selecting the total cost as the Objective . Then select the Savings Plan commit cells in By Changing Variable Cells . Do not select the total cell, only the $ value for specific instance family types: Optionally, you may add constraints for the Savings Plan commit cells. Windows: Mac OS Then click Solve and wait while the Solver calculates the savings plan commits that will minimize your overall costs. The solver may run a while, but when it finishes then save the results. Windows: Mac OS: The spreadsheet will then show the savings plan commits that minimize the total costs. In this case Savings Plans were able to reduce total costs by about 10%. Note that this is due to the very variable nature of the jobs. The more uniform usage you have, the more cost-effective Savings Plans will be. This graph shows the variability of the workload.","title":"Optimize Savings Plans Using Excel Solver"},{"location":"SlurmLogParser/","text":"Slurm SlurmLogParser.py parses the Slurm Accounting Database records, and its output is then used by JobAnalyzer.py for cost simulation. Video walkthrough The fastest way to learn about using HPC Cost Simulator SchedMD Slurm is to watch this short walkthrough. Modes of running the tool SlurmLogParser.py has 2 modes of operation, online and offline. Offline - Recommended In offline mode, the job completion data is extracted using sacct ahead of processing (not as part of the script execution). This allows processing to happen on a Linux machine not configured as part of the Slurm cluster (non production machine). When running in this mode, provide the file path to HCS using the --sacct-input-file argument. Online Online means HCS will call the sacct command directly, which requires the Linux machine to have sacct installed and configured as well as the permissions to access accounting records from all users . Running in this mode will store the output of the sacct command in a CSV file to allow additional analysis to be done offline (minimizing calls the Slurm head node). You need to provide the name of this CSV file using the --sacct-output-file argument. If the Slurm bin folder is not in your path, you will also need to provide the --slurm-root argument, pointing to the Slurm bin folder. Note : You can't use both --sacct-input-file and --sacct-output-file together. Collecting the data To see data collection command syntax, run: source setup.sh ./SlurmLogParser.py --show-data-collection-cmd As of this writing, the command is: sacct --allusers --parsable2 --noheader --format State,JobID,ReqCPUS,ReqMem,ReqNodes,Constraints,Submit,Eligible,Start,Elapsed,Suspended,End,ExitCode,DerivedExitCode,AllocNodes,NCPUS,MaxDiskRead,MaxDiskWrite,MaxPages,MaxRSS,MaxVMSize,CPUTime,UserCPU,SystemCPU,TotalCPU,Partition --starttime 1970-01-01T0:00:00 > SlurmAccounting.csv Parsing the Job Completion Data First you must source the setup script to activate the virtual environment and install all the dependencies in it. source setup.sh ./SlurmLogParser.py \\ --sacct-input-file SlurmAccounting.csv \\ --output-csv jobs.csv Full Syntax usage: SlurmLogParser.py [-h] (--show-data-collection-cmd | --sacct-output-file SACCT_OUTPUT_FILE | --sacct-input-file SACCT_INPUT_FILE) [--output-csv OUTPUT_CSV] [--slurm-root SLURM_ROOT] [--starttime STARTTIME] [--endtime ENDTIME] [--debug] Parse Slurm logs. optional arguments: -h, --help show this help message and exit --show-data-collection-cmd Show the command to create SACCT_OUTPUT_FILE. (default: False) --sacct-output-file SACCT_OUTPUT_FILE File where the output of sacct will be written. Cannot be used with --sacct-file. Required if --sacct-input- file not set. (default: None) --sacct-input-file SACCT_INPUT_FILE File with the output of sacct so can process sacct output offline. Cannot be used with --sacct-output- file. Required if --sacct-output-file not set. (default: None) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --slurm-root SLURM_ROOT Directory that contains the Slurm bin directory. (default: None) --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --debug, -d Enable debug mode (default: False) What's next? Once completed, you can run step #3 (cost simulation) by following the instructions for the JobAnalyzer . Since you already generated a CSV file, you will use JobAnalyzer with the csv option.","title":"Slurm"},{"location":"SlurmLogParser/#slurm","text":"SlurmLogParser.py parses the Slurm Accounting Database records, and its output is then used by JobAnalyzer.py for cost simulation.","title":"Slurm"},{"location":"SlurmLogParser/#video-walkthrough","text":"The fastest way to learn about using HPC Cost Simulator SchedMD Slurm is to watch this short walkthrough.","title":"Video walkthrough"},{"location":"SlurmLogParser/#modes-of-running-the-tool","text":"SlurmLogParser.py has 2 modes of operation, online and offline.","title":"Modes of running the tool"},{"location":"SlurmLogParser/#offline-recommended","text":"In offline mode, the job completion data is extracted using sacct ahead of processing (not as part of the script execution). This allows processing to happen on a Linux machine not configured as part of the Slurm cluster (non production machine). When running in this mode, provide the file path to HCS using the --sacct-input-file argument.","title":"Offline - Recommended"},{"location":"SlurmLogParser/#online","text":"Online means HCS will call the sacct command directly, which requires the Linux machine to have sacct installed and configured as well as the permissions to access accounting records from all users . Running in this mode will store the output of the sacct command in a CSV file to allow additional analysis to be done offline (minimizing calls the Slurm head node). You need to provide the name of this CSV file using the --sacct-output-file argument. If the Slurm bin folder is not in your path, you will also need to provide the --slurm-root argument, pointing to the Slurm bin folder. Note : You can't use both --sacct-input-file and --sacct-output-file together.","title":"Online"},{"location":"SlurmLogParser/#collecting-the-data","text":"To see data collection command syntax, run: source setup.sh ./SlurmLogParser.py --show-data-collection-cmd As of this writing, the command is: sacct --allusers --parsable2 --noheader --format State,JobID,ReqCPUS,ReqMem,ReqNodes,Constraints,Submit,Eligible,Start,Elapsed,Suspended,End,ExitCode,DerivedExitCode,AllocNodes,NCPUS,MaxDiskRead,MaxDiskWrite,MaxPages,MaxRSS,MaxVMSize,CPUTime,UserCPU,SystemCPU,TotalCPU,Partition --starttime 1970-01-01T0:00:00 > SlurmAccounting.csv","title":"Collecting the data"},{"location":"SlurmLogParser/#parsing-the-job-completion-data","text":"First you must source the setup script to activate the virtual environment and install all the dependencies in it. source setup.sh ./SlurmLogParser.py \\ --sacct-input-file SlurmAccounting.csv \\ --output-csv jobs.csv","title":"Parsing the Job Completion Data"},{"location":"SlurmLogParser/#full-syntax","text":"usage: SlurmLogParser.py [-h] (--show-data-collection-cmd | --sacct-output-file SACCT_OUTPUT_FILE | --sacct-input-file SACCT_INPUT_FILE) [--output-csv OUTPUT_CSV] [--slurm-root SLURM_ROOT] [--starttime STARTTIME] [--endtime ENDTIME] [--debug] Parse Slurm logs. optional arguments: -h, --help show this help message and exit --show-data-collection-cmd Show the command to create SACCT_OUTPUT_FILE. (default: False) --sacct-output-file SACCT_OUTPUT_FILE File where the output of sacct will be written. Cannot be used with --sacct-file. Required if --sacct-input- file not set. (default: None) --sacct-input-file SACCT_INPUT_FILE File with the output of sacct so can process sacct output offline. Cannot be used with --sacct-output- file. Required if --sacct-output-file not set. (default: None) --output-csv OUTPUT_CSV CSV file with parsed job completion records (default: None) --slurm-root SLURM_ROOT Directory that contains the Slurm bin directory. (default: None) --starttime STARTTIME Select jobs after the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --endtime ENDTIME Select jobs before the specified time. Format YYYY-MM- DDTHH:MM:SS (default: None) --debug, -d Enable debug mode (default: False)","title":"Full Syntax"},{"location":"SlurmLogParser/#whats-next","text":"Once completed, you can run step #3 (cost simulation) by following the instructions for the JobAnalyzer . Since you already generated a CSV file, you will use JobAnalyzer with the csv option.","title":"What's next?"},{"location":"UnderstandingCSVformat/","text":"Understanding the CSV format The CSV files generated by parsing LSF / Slurm / Accelerator job completion records all have the same structure. This simplifies the cost simulation, as only 1 input type is processed. You can also generate this CSV file yourself from any other scheduler and use JobAnalyzer.py to parse that, but you need to adhere to the structure below (The order of the fields in the CSV file do not matter, but the names of the fields do). CSV structure There is no standard for CSV files, but the JobAnalyzer.py expects the files to be written in the \"Microsoft Excel\" dialect, adn field names must be in the first row. Required Fields Field Type Description job_id string Job id num_cores int Total number of cores allocated to the job max_mem_gb float Total amount of memory allocated to the job in GB num_hosts int Number of hosts. In Slurm this is the number of nodes. The number of cores should be evenly divisible by the number of hosts. For LSF this is typically 1. submit_time datetime string Time that the job was submitted. All times are expected to be in the format YYYY - MM - DD T hh : mm : ss start_time datetime string Time that the job started. finish_time datetime string Time that the job finished. Optional Fields Field Type Description resource_request string Resources requested by the job. For LSF this is the effective resource request. For Slurm it is the job constraints. ineligible_pend_time timedelta string Duration that the job was ineligible to run. This is what LSF writes. Defaults to max(0, eligible_time - start_time). Format for timedelta strings is h : mm : ss . eligible_time datetime string Time that the job became eligible to run. Defaults to the start_time + ineligible_pend_time. requeue_time datetime string Time that the job was requeued. Currently not used. wait_time timedelta string Time that job was pending after it was eligible to run. Default start_time - eligible_time. run_time timedelta string Time that the job ran. Default: finish_time - start_time exit_status int Effective return code of the job. Default: 0 ru_majflt float Number of page faults ru_maxrss float Maximum shared text size ru_minflt float Number of page reclaims ru_msgsnd float Number of System V IPC messages sent ru_msgrcv float Number of messages received ru_nswap float Number of times the process was swapped out ru_inblock float Number of block input operations ru_oublock float Number of block output operations ru_stime float System time used ru_utime float User time used","title":"Understanding the CSV format"},{"location":"UnderstandingCSVformat/#understanding-the-csv-format","text":"The CSV files generated by parsing LSF / Slurm / Accelerator job completion records all have the same structure. This simplifies the cost simulation, as only 1 input type is processed. You can also generate this CSV file yourself from any other scheduler and use JobAnalyzer.py to parse that, but you need to adhere to the structure below (The order of the fields in the CSV file do not matter, but the names of the fields do).","title":"Understanding the CSV format"},{"location":"UnderstandingCSVformat/#csv-structure","text":"There is no standard for CSV files, but the JobAnalyzer.py expects the files to be written in the \"Microsoft Excel\" dialect, adn field names must be in the first row.","title":"CSV structure"},{"location":"UnderstandingCSVformat/#required-fields","text":"Field Type Description job_id string Job id num_cores int Total number of cores allocated to the job max_mem_gb float Total amount of memory allocated to the job in GB num_hosts int Number of hosts. In Slurm this is the number of nodes. The number of cores should be evenly divisible by the number of hosts. For LSF this is typically 1. submit_time datetime string Time that the job was submitted. All times are expected to be in the format YYYY - MM - DD T hh : mm : ss start_time datetime string Time that the job started. finish_time datetime string Time that the job finished.","title":"Required Fields"},{"location":"UnderstandingCSVformat/#optional-fields","text":"Field Type Description resource_request string Resources requested by the job. For LSF this is the effective resource request. For Slurm it is the job constraints. ineligible_pend_time timedelta string Duration that the job was ineligible to run. This is what LSF writes. Defaults to max(0, eligible_time - start_time). Format for timedelta strings is h : mm : ss . eligible_time datetime string Time that the job became eligible to run. Defaults to the start_time + ineligible_pend_time. requeue_time datetime string Time that the job was requeued. Currently not used. wait_time timedelta string Time that job was pending after it was eligible to run. Default start_time - eligible_time. run_time timedelta string Time that the job ran. Default: finish_time - start_time exit_status int Effective return code of the job. Default: 0 ru_majflt float Number of page faults ru_maxrss float Maximum shared text size ru_minflt float Number of page reclaims ru_msgsnd float Number of System V IPC messages sent ru_msgrcv float Number of messages received ru_nswap float Number of times the process was swapped out ru_inblock float Number of block input operations ru_oublock float Number of block output operations ru_stime float System time used ru_utime float User time used","title":"Optional Fields"},{"location":"UpdateInstanceDatabase/","text":"Updating the Instance Type Information While we update the HCS Instance Type Information with each new release, you can update it yourself to get updated instance types and prices for one region or all regions. The Instance Type Information is stored in JSON format in instance_type_info.json Note : Running the script without specifying a region will collect the data for all instance types across all available regions, which may take a few minutes to complete. Prerequisites To be able to update the EC2 Instance Type Information you need: AWS CLI installed AWS CLI Profile with the permissions shown below Required AWS permissions { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"GetEC2InstanceTypeInfo\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:DescribeInstanceTypes\", \"ec2:DescribeRegions\", \"ec2:DescribeSpotPriceHistory\", \"pricing:GetProducts\" ], \"Resource\": \"*\" } ] } Running the update Once your source setup.sh to setup the virtual environment, you can run get_instance_type_info.py to update the Instance Database: source setup.sh rm instance_type_info.json ./get_ec2_instance_info.py --input instance_type_info.json --region eu-west-1 Note: If the script fails to complete for all regions, you can restart it without losing the data already collected by rerunning it with the --input parameter and providing the partial output from the previous run ( instance_type_info.json ) Full Syntax: usage: get_ec2_instance_info.py [-h] [--region REGION] [--input INPUT] [--output-csv OUTPUT_CSV] [--debug] Get EC2 instance pricing info. optional arguments: -h, --help show this help message and exit --region REGION, -r REGION AWS region(s) to get info for. (default: []) --input INPUT, -i INPUT JSON input file. Reads existing info from previous runs. Can speed up rerun if a region failed. (default: None) --output-csv OUTPUT_CSV, -o OUTPUT_CSV CSV output file. Default: instance_type_info.csv (default: None) --debug, -d Enable debug messages (default: False)","title":"Updating the Instance Type Information"},{"location":"UpdateInstanceDatabase/#updating-the-instance-type-information","text":"While we update the HCS Instance Type Information with each new release, you can update it yourself to get updated instance types and prices for one region or all regions. The Instance Type Information is stored in JSON format in instance_type_info.json Note : Running the script without specifying a region will collect the data for all instance types across all available regions, which may take a few minutes to complete.","title":"Updating the Instance Type Information"},{"location":"UpdateInstanceDatabase/#prerequisites","text":"To be able to update the EC2 Instance Type Information you need: AWS CLI installed AWS CLI Profile with the permissions shown below","title":"Prerequisites"},{"location":"UpdateInstanceDatabase/#required-aws-permissions","text":"{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"GetEC2InstanceTypeInfo\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:DescribeInstanceTypes\", \"ec2:DescribeRegions\", \"ec2:DescribeSpotPriceHistory\", \"pricing:GetProducts\" ], \"Resource\": \"*\" } ] }","title":"Required AWS permissions"},{"location":"UpdateInstanceDatabase/#running-the-update","text":"Once your source setup.sh to setup the virtual environment, you can run get_instance_type_info.py to update the Instance Database: source setup.sh rm instance_type_info.json ./get_ec2_instance_info.py --input instance_type_info.json --region eu-west-1 Note: If the script fails to complete for all regions, you can restart it without losing the data already collected by rerunning it with the --input parameter and providing the partial output from the previous run ( instance_type_info.json )","title":"Running the update"},{"location":"UpdateInstanceDatabase/#full-syntax","text":"usage: get_ec2_instance_info.py [-h] [--region REGION] [--input INPUT] [--output-csv OUTPUT_CSV] [--debug] Get EC2 instance pricing info. optional arguments: -h, --help show this help message and exit --region REGION, -r REGION AWS region(s) to get info for. (default: []) --input INPUT, -i INPUT JSON input file. Reads existing info from previous runs. Can speed up rerun if a region failed. (default: None) --output-csv OUTPUT_CSV, -o OUTPUT_CSV CSV output file. Default: instance_type_info.csv (default: None) --debug, -d Enable debug messages (default: False)","title":"Full Syntax:"},{"location":"config/","text":"Configuration All configuration parameters are stored in config.yml . The schema of the configuration file is contained in config_schema.yml and contains a list of all the options. See comments within the files for details of each parameter's use. Key configuration parameters that you may want to change include. Parameter Default Description instance_mapping: region_name eu-west-1 The region where the AWS instances will run. Since instance availability and prices vary by region, this has direct impact on costs instance_mapping: instance_prefix_list [c6i, m5., r5., c5., z1d, x2i] Instance types to be used during the analysis consumption_model_mapping: maximum_minutes_for_spot 60 Threshold for using on-demand instances instead of spot instances. consumption_model_mapping: ec2_savings_plan_duration 3 Duration of the savings plan. Valid values: [1, 3] consumption_model_mapping: ec2_savings_plan_payment_option 'All Upfront' Payment option. Valid values: ['All Upfront', 'Partial Upfront', 'No Upfront'] consumption_model_mapping: 3 Duration of the savings plan. Valid values: [1, 3] consumption_model_mapping: 'All Upfront' Payment option. Valid values: ['All Upfront', 'Partial Upfront', 'No Upfront']","title":"Config"},{"location":"config/#configuration","text":"All configuration parameters are stored in config.yml . The schema of the configuration file is contained in config_schema.yml and contains a list of all the options. See comments within the files for details of each parameter's use. Key configuration parameters that you may want to change include. Parameter Default Description instance_mapping: region_name eu-west-1 The region where the AWS instances will run. Since instance availability and prices vary by region, this has direct impact on costs instance_mapping: instance_prefix_list [c6i, m5., r5., c5., z1d, x2i] Instance types to be used during the analysis consumption_model_mapping: maximum_minutes_for_spot 60 Threshold for using on-demand instances instead of spot instances. consumption_model_mapping: ec2_savings_plan_duration 3 Duration of the savings plan. Valid values: [1, 3] consumption_model_mapping: ec2_savings_plan_payment_option 'All Upfront' Payment option. Valid values: ['All Upfront', 'Partial Upfront', 'No Upfront'] consumption_model_mapping: 3 Duration of the savings plan. Valid values: [1, 3] consumption_model_mapping: 'All Upfront' Payment option. Valid values: ['All Upfront', 'Partial Upfront', 'No Upfront']","title":"Configuration"},{"location":"model-scheduling/","text":"Model How Jobs Would Be Scheduled on AWS Once you have parsed the scheduler logs, you can model how the same jobs might run on a scalable AWS compute cluster. The ModelComputeCluster.py script will model how the jobs would run on an ideal AWS cluster with no scaling limitations. The script models the jobs starting at the point that they are eligible to start and models the instances required to run those jobs. The output of the model is the total compute costs for running the jobs and the amount of wait time saved by running on AWS. Initially, the model will not have any instances running. The jobs will be sorted by their eligible time and when that time arrives the scheduler will allocate instances for the job. The resource allocation strategy is to pick the least expensive instance that has the required number of cores and memory available. If no instance is found, then a new instance will be started in the model for the job to run on. The model will start the least expensive instance type that the job can run on. When the model adds a new instance it is not available to run jobs for a configurable bootup time (default 2 minutes). When an instance is idle for longer than a configurable idle time (default 5 minutes), then it will removed from the model. The model tracks the costs for bootup and idle times and the time spent running jobs. The model also reports hourly spot and on-demand costs. Since an instance can run multiple jobs at the same time, we have to decide whether costs need to be allocated to individual jobs. This is unnecessary since all we are concerned with are the overall compute costs and do not break out job costs at the end. If we did want to allocate job costs it is complicated because some jobs may be core limited, others memory limited, and there are boot and idle times that have to be allocated. An instance's cost is allocated to a job based on the percentage of cores and memory used by the job relative to the total percentage of allocated cores and memory. Since jobs start and stop at different times, the cost of a job will vary over time based on the jobs that are running concurrently with it. For example, if a job is the only job running for the first 10 minutes, then it is charged the full cost of the instance for those 10 minutes. If another job starts with the same resource requirements, then 50% of the instance costs will be allocated to each job while the jobs are running concurrently. The jobs' costs are recalculated each time a job starts or finishes on the instance. The percentage of the instance's resources that are used by multiple jobs is not straightforward because jobs can be core limited or memory limited. If cores are the limiting resource, then memory will be underutilized. If memory is the limiting resource, then cores will be underutilized. So, the first step in allocating instance costs is to determine the limiting resource by calculating the percentage of cores and memory allocated to all the jobs running on the instance and picking the resource with the higher allocation. For example, if all of the cores are allocated then the costs will be allocated to each job based on the percentage of allocated cores that it is using. If 90% of the memory is allocated and only 25% of the cores, then memory is considered the limiting resource and the costs will be allocated to a job based on the percentage of total allocated memory that it is using. The model can also be configured with reserved instances (RIs). RIs in the model will always be available to run jobs. The model can also be configured with on-premises servers. The configuration must include an inventory of the server types with the number of cores, amount of memory, and hourly cost. The model can also use spot pricing on the instances that it uses. You can configure the profile of jobs that will utilize spot based on the runtime of the job. For example, you can configure the model to use spot pricing instead of on-demand pricing for all jobs that run for less than 5 hours. The model will also randomly terminate spot jobs and requeue any jobs that were running on the instance when it was terminated. The model has a spot termination probability per minute that you can configure. Note that this is for modeling purposes only and that AWS provides no SLA for spot termination rates. The actual termination probability can vary significantly based on availability in the spot pools which can vary significantly over time. The default probability is chosen so that a job that runs for 5 hours has a 5% chance of being terminated which in line with real world experiences, although, again, this may differ markedly in real life. Model Configuration ComputeClusterModel: BootTime: '2:00' IdleTime: '4:00' SchedulingFrequency: '0:00' ShareInstances: False ReservedInstances: - InstanceType: Count Configure On-Premises Servers (Optional) Model RIs (Optional) Design Main Sort all jobs by eligible_time Create ComputeClusterModel while get_job() if job.eligible_time > current_time: computeClusterModel.advance_time(job.eligible_time) computeClusterModel.schedule_job(job) ComputeClusterModel List of ComputeInstance sorted by cost. Group by PricingOption and InstanceType? For same cost sorted reverse by Utilization HourlyCost: Update when instance terminated or at the end of an hour. Cost at termination is only the cost since the previous hour. Save Spot and OnDemand costs separately Save number of Spot and OnDemand core hours Save OnDemand costs split out by instance family Event list sorted by datetime JobFinishEvent: Includes the instance running the job. InstanceTerminateEvent: Set when last job on instance terminates. Cancel instance termination if job has been allocated. EndOfHourEvent: Update HourlyCost with cost of instances still running at the end of the hour. Create next EndOfHourEvent. advance_time(new_time: datetime): Process all events up to new_time schedule_job: allocate to existing instance or to a new instance. Create JobFinish event for the job. If new instance JobFinish event should include boot time. finish_job: Delete job from instance. If no more jobs then create TerminateInstance event. terminate_instance: If no jobs running then update HourlyCost and remove instance from list. ComputeInstance EC2/OnPrem InstanceType NumberOfCores ThreadsPerCore HTEnabled Memory (GB) PricingOption: OnDemand or Spot HourlyCost State: Booting, Running, Idle List of Jobs Available cores Available memory Core Utilization Memory Utilization Instance Utilization (Max of core/memory utilization)","title":"Model How Jobs Would Be Scheduled on AWS"},{"location":"model-scheduling/#model-how-jobs-would-be-scheduled-on-aws","text":"Once you have parsed the scheduler logs, you can model how the same jobs might run on a scalable AWS compute cluster. The ModelComputeCluster.py script will model how the jobs would run on an ideal AWS cluster with no scaling limitations. The script models the jobs starting at the point that they are eligible to start and models the instances required to run those jobs. The output of the model is the total compute costs for running the jobs and the amount of wait time saved by running on AWS. Initially, the model will not have any instances running. The jobs will be sorted by their eligible time and when that time arrives the scheduler will allocate instances for the job. The resource allocation strategy is to pick the least expensive instance that has the required number of cores and memory available. If no instance is found, then a new instance will be started in the model for the job to run on. The model will start the least expensive instance type that the job can run on. When the model adds a new instance it is not available to run jobs for a configurable bootup time (default 2 minutes). When an instance is idle for longer than a configurable idle time (default 5 minutes), then it will removed from the model. The model tracks the costs for bootup and idle times and the time spent running jobs. The model also reports hourly spot and on-demand costs. Since an instance can run multiple jobs at the same time, we have to decide whether costs need to be allocated to individual jobs. This is unnecessary since all we are concerned with are the overall compute costs and do not break out job costs at the end. If we did want to allocate job costs it is complicated because some jobs may be core limited, others memory limited, and there are boot and idle times that have to be allocated. An instance's cost is allocated to a job based on the percentage of cores and memory used by the job relative to the total percentage of allocated cores and memory. Since jobs start and stop at different times, the cost of a job will vary over time based on the jobs that are running concurrently with it. For example, if a job is the only job running for the first 10 minutes, then it is charged the full cost of the instance for those 10 minutes. If another job starts with the same resource requirements, then 50% of the instance costs will be allocated to each job while the jobs are running concurrently. The jobs' costs are recalculated each time a job starts or finishes on the instance. The percentage of the instance's resources that are used by multiple jobs is not straightforward because jobs can be core limited or memory limited. If cores are the limiting resource, then memory will be underutilized. If memory is the limiting resource, then cores will be underutilized. So, the first step in allocating instance costs is to determine the limiting resource by calculating the percentage of cores and memory allocated to all the jobs running on the instance and picking the resource with the higher allocation. For example, if all of the cores are allocated then the costs will be allocated to each job based on the percentage of allocated cores that it is using. If 90% of the memory is allocated and only 25% of the cores, then memory is considered the limiting resource and the costs will be allocated to a job based on the percentage of total allocated memory that it is using. The model can also be configured with reserved instances (RIs). RIs in the model will always be available to run jobs. The model can also be configured with on-premises servers. The configuration must include an inventory of the server types with the number of cores, amount of memory, and hourly cost. The model can also use spot pricing on the instances that it uses. You can configure the profile of jobs that will utilize spot based on the runtime of the job. For example, you can configure the model to use spot pricing instead of on-demand pricing for all jobs that run for less than 5 hours. The model will also randomly terminate spot jobs and requeue any jobs that were running on the instance when it was terminated. The model has a spot termination probability per minute that you can configure. Note that this is for modeling purposes only and that AWS provides no SLA for spot termination rates. The actual termination probability can vary significantly based on availability in the spot pools which can vary significantly over time. The default probability is chosen so that a job that runs for 5 hours has a 5% chance of being terminated which in line with real world experiences, although, again, this may differ markedly in real life.","title":"Model How Jobs Would Be Scheduled on AWS"},{"location":"model-scheduling/#model-configuration","text":"ComputeClusterModel: BootTime: '2:00' IdleTime: '4:00' SchedulingFrequency: '0:00' ShareInstances: False ReservedInstances: - InstanceType: Count","title":"Model Configuration"},{"location":"model-scheduling/#configure-on-premises-servers-optional","text":"","title":"Configure On-Premises Servers (Optional)"},{"location":"model-scheduling/#model-ris-optional","text":"","title":"Model RIs (Optional)"},{"location":"model-scheduling/#design","text":"","title":"Design"},{"location":"model-scheduling/#main","text":"Sort all jobs by eligible_time Create ComputeClusterModel while get_job() if job.eligible_time > current_time: computeClusterModel.advance_time(job.eligible_time) computeClusterModel.schedule_job(job)","title":"Main"},{"location":"model-scheduling/#computeclustermodel","text":"List of ComputeInstance sorted by cost. Group by PricingOption and InstanceType? For same cost sorted reverse by Utilization HourlyCost: Update when instance terminated or at the end of an hour. Cost at termination is only the cost since the previous hour. Save Spot and OnDemand costs separately Save number of Spot and OnDemand core hours Save OnDemand costs split out by instance family Event list sorted by datetime JobFinishEvent: Includes the instance running the job. InstanceTerminateEvent: Set when last job on instance terminates. Cancel instance termination if job has been allocated. EndOfHourEvent: Update HourlyCost with cost of instances still running at the end of the hour. Create next EndOfHourEvent. advance_time(new_time: datetime): Process all events up to new_time schedule_job: allocate to existing instance or to a new instance. Create JobFinish event for the job. If new instance JobFinish event should include boot time. finish_job: Delete job from instance. If no more jobs then create TerminateInstance event. terminate_instance: If no jobs running then update HourlyCost and remove instance from list.","title":"ComputeClusterModel"},{"location":"model-scheduling/#computeinstance","text":"EC2/OnPrem InstanceType NumberOfCores ThreadsPerCore HTEnabled Memory (GB) PricingOption: OnDemand or Spot HourlyCost State: Booting, Running, Idle List of Jobs Available cores Available memory Core Utilization Memory Utilization Instance Utilization (Max of core/memory utilization)","title":"ComputeInstance"}]}